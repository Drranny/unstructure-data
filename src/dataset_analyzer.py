"""
ë°ì´í„°ì…‹ ë°°ì¹˜ ë¶„ì„ ëª¨ë“ˆ
CIFAR-10, TID2013 ë“±ì˜ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë°°ì¹˜ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
"""
import numpy as np
from PIL import Image
import imagehash
from typing import List, Dict
from src.image_quality import analyze_image_quality, calculate_duplication_score
from src.text_quality import analyze_text_quality
from src.utils import calc_total_score

def analyze_dataset_images(images: List[Image.Image], max_samples: int = 100) -> Dict:
    """
    ì—¬ëŸ¬ ì´ë¯¸ì§€ì˜ í’ˆì§ˆì„ ë°°ì¹˜ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        images: PIL Image ê°ì²´ ë¦¬ìŠ¤íŠ¸
        max_samples: ìµœëŒ€ ë¶„ì„í•  ì´ë¯¸ì§€ ê°œìˆ˜ (ì„±ëŠ¥ ê³ ë ¤)
        
    Returns:
        dict: ì „ì²´ ë°ì´í„°ì…‹ì˜ í’ˆì§ˆ í†µê³„
    """
    if len(images) == 0:
        return {
            "ì´ ì´ë¯¸ì§€ ìˆ˜": 0,
            "í‰ê·  í•´ìƒë„": 0.0,
            "í‰ê·  ì„ ëª…ë„": 0.0,
            "í‰ê·  ë…¸ì´ì¦ˆ": 0.0,
            "í‰ê·  ì¤‘ë³µë„": 0.0,
            "í‰ê·  ì¢…í•© ì ìˆ˜": 0.0,
        }
    
    # ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´ ì¼ë¶€ë§Œ)
    # ì°¸ê³ : random.sampleì„ ì‚¬ìš©í•˜ì—¬ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ë¯€ë¡œ,
    # í•´ìƒë„ê°€ ë‹¤ë¥¸ ì´ë¯¸ì§€ë“¤ì´ ê³¨ê³ ë£¨ ì„ íƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    is_single_image = (len(images) == 1)
    
    original_count = len(images)
    if len(images) > max_samples:
        import random
        images = random.sample(images, max_samples)
    
    all_scores = {
        "í•´ìƒë„": [],
        "ì„ ëª…ë„": [],
        "ë…¸ì´ì¦ˆ": [],
        "ì¤‘ë³µë„": [],
        "ì¢…í•©ì ìˆ˜": []
    }
    
    # ì‹¤ì œ í•´ìƒë„ ì •ë³´ ì €ì¥ (width x height)
    actual_resolutions = []  # (width, height) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
    
    # ê°œë³„ ì´ë¯¸ì§€ ì ìˆ˜ ì €ì¥
    individual_scores = []  # ê° ì´ë¯¸ì§€ì˜ ê°œë³„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
    
    image_hashes = []
    
    # ê° ì´ë¯¸ì§€ ë¶„ì„
    for img in images:
        scores = analyze_image_quality(img, is_single_image=is_single_image)
        
        score_count = len(scores) # ë‹¨ì¼ ë¶„ì„ ì‹œ 3, ë°°ì¹˜ ë¶„ì„ ì‹œ 4
        
        if score_count == 3: # ë‹¨ì¼ ë¶„ì„ (ì¤‘ë³µë„ ì œì™¸)
            total = (scores["í•´ìƒë„"] + scores["ì„ ëª…ë„"] + (1 - scores["ë…¸ì´ì¦ˆ"])) / 3
            dup_score = "N/A"
        else: # ë°°ì¹˜ ë¶„ì„ (ì¤‘ë³µë„ í¬í•¨)
            total = (scores["í•´ìƒë„"] + scores["ì„ ëª…ë„"] + (1 - scores["ë…¸ì´ì¦ˆ"]) + (1 - scores["ì¤‘ë³µë„"])) / 4
            dup_score = scores["ì¤‘ë³µë„"]
        
        all_scores["í•´ìƒë„"].append(scores["í•´ìƒë„"])
        all_scores["ì„ ëª…ë„"].append(scores["ì„ ëª…ë„"])
        all_scores["ë…¸ì´ì¦ˆ"].append(scores["ë…¸ì´ì¦ˆ"])
        
        if not is_single_image:
             # ì¤‘ë³µë„ í•­ëª©ì€ ë°°ì¹˜ ë¶„ì„ì¼ ë•Œë§Œ scoresì— ìˆìœ¼ë¯€ë¡œ ì €ì¥
            all_scores["ì¤‘ë³µë„"].append(scores["ì¤‘ë³µë„"])
            
        all_scores["ì¢…í•©ì ìˆ˜"].append(total)
        
        # ê°œë³„ ì ìˆ˜ ì €ì¥
        individual_scores.append({
            "í•´ìƒë„": round(scores["í•´ìƒë„"], 3),
            "ì„ ëª…ë„": round(scores["ì„ ëª…ë„"], 3),
            "ë…¸ì´ì¦ˆ": round(scores["ë…¸ì´ì¦ˆ"], 3),
            "ì¤‘ë³µë„": dup_score if isinstance(dup_score, str) else round(dup_score, 3),
            "ì¢…í•©ì ìˆ˜": round(total, 3),
        })
        
        # ì‹¤ì œ í•´ìƒë„ ì €ì¥ (width x height)
        actual_resolutions.append((img.width, img.height))
        
        # ì¤‘ë³µë„ ê³„ì‚°ì„ ìœ„í•œ í•´ì‹œ ì €ì¥
        if not is_single_image:
            image_hashes.append(imagehash.average_hash(img))
    
        
    # ì¢…í•© ì ìˆ˜ ì¬ê³„ì‚°: ê°œë³„ í‰ê· ì„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°
    avg_resolution = np.mean(all_scores["í•´ìƒë„"])
    avg_sharpness  = np.mean(all_scores["ì„ ëª…ë„"])
    avg_noise      = np.mean(all_scores["ë…¸ì´ì¦ˆ"])
    
    if not is_single_image and len(image_hashes) > 1:
        # ë°°ì¹˜ ë¶„ì„: ì¤‘ë³µë„ ê³„ì‚° ë° 4ê°œ ì§€í‘œ ê¸°ë°˜ ìµœì¢… ì¢…í•© ì ìˆ˜ ê³„ì‚°
        avg_dup = calculate_duplication_score(image_hashes)
        avg_total = np.mean(all_scores["ì¢…í•©ì ìˆ˜"])
        report_avg_dup = round(avg_dup, 3) 
    else:
        # ë‹¨ì¼ ë¶„ì„: ì¤‘ë³µë„ N/A ì²˜ë¦¬ ë° 3ê°œ ì§€í‘œ ê¸°ë°˜ ìµœì¢… ì¢…í•© ì ìˆ˜ ê³„ì‚°
        avg_total = np.mean(all_scores["ì¢…í•©ì ìˆ˜"])
        report_avg_dup = "N/A" 
    
    # í•´ìƒë„ í†µê³„ ê³„ì‚°
    widths = [r[0] for r in actual_resolutions]
    heights = [r[1] for r in actual_resolutions]
    total_pixels = [w * h for w, h in actual_resolutions]
    
    # í†µê³„ ê³„ì‚°
    result = {
        "ì´ ì´ë¯¸ì§€ ìˆ˜": len(images),
        "ì›ë³¸ ë°ì´í„°ì…‹ í¬ê¸°": original_count if original_count > len(images) else len(images),
        "ìƒ˜í”Œë§ ì—¬ë¶€": "ì˜ˆ" if original_count > len(images) else "ì•„ë‹ˆì˜¤",
        "í‰ê·  í•´ìƒë„": round(avg_resolution, 3),
        "í‰ê·  ì„ ëª…ë„": round(avg_sharpness, 3),
        "í‰ê·  ë…¸ì´ì¦ˆ": round(avg_noise, 3),
        "í‰ê·  ì¤‘ë³µë„": report_avg_dup,
        "í‰ê·  ì¢…í•© ì ìˆ˜": round(avg_total, 3),
        "ìµœì†Œ ì¢…í•© ì ìˆ˜": round(np.min(all_scores["ì¢…í•©ì ìˆ˜"]), 3),
        "ìµœëŒ€ ì¢…í•© ì ìˆ˜": round(np.max(all_scores["ì¢…í•©ì ìˆ˜"]), 3),
        "í‘œì¤€í¸ì°¨": round(np.std(all_scores["ì¢…í•©ì ìˆ˜"]), 3) if len(all_scores["ì¢…í•©ì ìˆ˜"]) > 1 else 0.0,
        
        # ì‹¤ì œ í•´ìƒë„ ì •ë³´ ì¶”ê°€
        "í•´ìƒë„ ë¶„í¬": {
            "ìµœì†Œ": f"{min(widths)}x{min(heights)}",
            "ìµœëŒ€": f"{max(widths)}x{max(heights)}",
            "í‰ê· ": f"{int(np.mean(widths))}x{int(np.mean(heights))}",
            "ì¤‘ì•™ê°’": f"{int(np.median(widths))}x{int(np.median(heights))}",
            "í‰ê·  í”½ì…€ ìˆ˜": f"{int(np.mean(total_pixels)):,}",
        },
        "í•´ìƒë„ ëª©ë¡": [f"{w}x{h}" for w, h in actual_resolutions],  # ì„ íƒëœ ì´ë¯¸ì§€ë“¤ì˜ ì‹¤ì œ í•´ìƒë„
        "ê°œë³„ ì ìˆ˜": individual_scores,  # ê° ì´ë¯¸ì§€ì˜ ê°œë³„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
    }
    
    return result

def analyze_dataset_texts(texts: List[str], max_samples: int = 100) -> Dict:
    """
    ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ ë°°ì¹˜ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        texts: í…ìŠ¤íŠ¸ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        max_samples: ìµœëŒ€ ë¶„ì„í•  í…ìŠ¤íŠ¸ ê°œìˆ˜ (ì„±ëŠ¥ ê³ ë ¤)
        
    Returns:
        dict: ì „ì²´ ë°ì´í„°ì…‹ì˜ í’ˆì§ˆ í†µê³„
    """
    if len(texts) == 0:
        return {
            "ì´ í…ìŠ¤íŠ¸ ìˆ˜": 0,
            "í‰ê·  ì •í™•ì„±": 0.0,
            "í‰ê·  ì¤‘ë³µë„": 0.0,
            "í‰ê·  ì™„ì „ì„±": 0.0,
            "í‰ê·  ì¢…í•© ì ìˆ˜": 0.0,
        }
    
    # ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´ ì¼ë¶€ë§Œ)
    if len(texts) > max_samples:
        import random
        texts = random.sample(texts, max_samples)
    
    all_scores = {
        "ì •í™•ì„±": [],
        "ì¤‘ë³µë„": [],
        "ì™„ì „ì„±": [],
        "ì¢…í•©ì ìˆ˜": []
    }
    
    # ê°œë³„ í…ìŠ¤íŠ¸ ì ìˆ˜ ì €ì¥
    individual_scores = []  # ê° í…ìŠ¤íŠ¸ì˜ ê°œë³„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
    
    # ê° í…ìŠ¤íŠ¸ ë¶„ì„
    for text in texts:
        if not text or len(text.strip()) == 0:
            continue
            
        scores = analyze_text_quality(text)
        total = calc_total_score(scores)
        
        accuracy = scores["ì •í™•ì„±(ì˜¤íƒˆìë¹„ìœ¨)"]
        duplication = scores["ì¤‘ë³µë„(ìœ ì‚¬ë„ì—­ë¹„ìœ¨)"]
        completeness = scores["ì™„ì „ì„±(ë¬¸ì¥ì¶©ì‹¤ë„)"]
        
        all_scores["ì •í™•ì„±"].append(accuracy)
        all_scores["ì¤‘ë³µë„"].append(duplication)
        all_scores["ì™„ì „ì„±"].append(completeness)
        all_scores["ì¢…í•©ì ìˆ˜"].append(total)
        
        # ê°œë³„ ì ìˆ˜ ì €ì¥
        individual_scores.append({
            "ì •í™•ì„±": round(accuracy, 3),
            "ì¤‘ë³µë„": round(duplication, 3),
            "ì™„ì „ì„±": round(completeness, 3),
            "ì¢…í•©ì ìˆ˜": round(total, 3),
        })
    
    if len(all_scores["ì¢…í•©ì ìˆ˜"]) == 0:
        return {
            "ì´ í…ìŠ¤íŠ¸ ìˆ˜": 0,
            "í‰ê·  ì •í™•ì„±": 0.0,
            "í‰ê·  ì¤‘ë³µë„": 0.0,
            "í‰ê·  ì™„ì „ì„±": 0.0,
            "í‰ê·  ì¢…í•© ì ìˆ˜": 0.0,
        }
    
    # í†µê³„ ê³„ì‚°
    result = {
        "ì´ í…ìŠ¤íŠ¸ ìˆ˜": len(all_scores["ì¢…í•©ì ìˆ˜"]),
        "í‰ê·  ì •í™•ì„±": round(np.mean(all_scores["ì •í™•ì„±"]), 3),
        "í‰ê·  ì¤‘ë³µë„": round(np.mean(all_scores["ì¤‘ë³µë„"]), 3),
        "í‰ê·  ì™„ì „ì„±": round(np.mean(all_scores["ì™„ì „ì„±"]), 3),
        "í‰ê·  ì¢…í•© ì ìˆ˜": round(np.mean(all_scores["ì¢…í•©ì ìˆ˜"]), 3),
        "ìµœì†Œ ì¢…í•© ì ìˆ˜": round(np.min(all_scores["ì¢…í•©ì ìˆ˜"]), 3),
        "ìµœëŒ€ ì¢…í•© ì ìˆ˜": round(np.max(all_scores["ì¢…í•©ì ìˆ˜"]), 3),
        "í‘œì¤€í¸ì°¨": round(np.std(all_scores["ì¢…í•©ì ìˆ˜"]), 3),
        "ê°œë³„ ì ìˆ˜": individual_scores,  # ê° í…ìŠ¤íŠ¸ì˜ ê°œë³„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
    }
    
    return result

def load_cifar10(num_samples: int = 100):
    """
    CIFAR-10 ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        num_samples: ë¡œë“œí•  ìƒ˜í”Œ ê°œìˆ˜
        
    Returns:
        List[PIL.Image]: ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
    """
    try:
        from torchvision import datasets
        from torchvision import transforms
        
        # CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ
        transform = transforms.Compose([
            transforms.ToPILImage()
        ])
        
        dataset = datasets.CIFAR10(
            root='./data', 
            train=True, 
            download=True, 
            transform=None
        )
        
        images = []
        for i in range(min(num_samples, len(dataset))):
            img, _ = dataset[i]
            if isinstance(img, Image.Image):
                images.append(img)
            else:
                # numpy arrayë‚˜ tensorì¸ ê²½ìš° ë³€í™˜
                img_pil = Image.fromarray(np.array(img))
                images.append(img_pil)
        
        return images
    
    except ImportError:
        raise ImportError("torchvisionì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install torchvision ì‹¤í–‰í•˜ì„¸ìš”.")
    except Exception as e:
        raise Exception(f"CIFAR-10 ë¡œë“œ ì‹¤íŒ¨: {e}")

def load_tid2013(num_samples: int = 100, custom_path: str = None):
    """
    TID2013 ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        num_samples: ë¡œë“œí•  ìƒ˜í”Œ ê°œìˆ˜
        custom_path: ì»¤ìŠ¤í…€ ê²½ë¡œ ì§€ì • (ì„ íƒì‚¬í•­)
        
    Returns:
        List[PIL.Image]: ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
    """
    import os
    import glob
    
    # TID2013ì€ ì§ì ‘ ë‹¤ìš´ë¡œë“œê°€ ì–´ë ¤ìš°ë¯€ë¡œ ë¡œì»¬ ê²½ë¡œì—ì„œ ë¡œë“œ
    # ë˜ëŠ” ì»¤ìŠ¤í…€ ê²½ë¡œ ì‚¬ìš©
    
    # ê°€ëŠ¥í•œ ê²½ë¡œë“¤
    if custom_path:
        possible_paths = [custom_path]
    else:
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ìƒì„±
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        possible_paths = [
            os.path.join(project_root, "data", "TID2013"),
            os.path.join(project_root, "TID2013"),
            os.path.join(".", "data", "TID2013"),
            os.path.join(".", "TID2013"),
            os.path.join("/", "data", "TID2013"),
            os.path.expanduser("~/TID2013"),
            os.path.expanduser("~/data/TID2013"),
        ]
    
    images = []
    
    for base_path in possible_paths:
        if not os.path.exists(base_path):
            continue
            
        # TID2013 êµ¬ì¡° í™•ì¸
        # 1. reference_images í´ë” ì•ˆì˜ ì´ë¯¸ì§€
        ref_path = os.path.join(base_path, "reference_images")
        if os.path.exists(ref_path):
            img_files = []
            img_files.extend(glob.glob(os.path.join(ref_path, "*.bmp")))
            img_files.extend(glob.glob(os.path.join(ref_path, "*.png")))
            img_files.extend(glob.glob(os.path.join(ref_path, "*.jpg")))
            img_files.extend(glob.glob(os.path.join(ref_path, "*.jpeg")))
            
            if img_files:
                for i, img_file in enumerate(sorted(img_files)[:num_samples]):
                    try:
                        img = Image.open(img_file)
                        images.append(img)
                    except Exception as e:
                        print(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_file}, {e}")
                        continue
                
                if images:
                    break
        
        # 2. ì§ì ‘ ì´ë¯¸ì§€ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°
        img_files_direct = []
        img_files_direct.extend(glob.glob(os.path.join(base_path, "*.bmp")))
        img_files_direct.extend(glob.glob(os.path.join(base_path, "*.png")))
        img_files_direct.extend(glob.glob(os.path.join(base_path, "*.jpg")))
        img_files_direct.extend(glob.glob(os.path.join(base_path, "*.jpeg")))
        
        if img_files_direct:
            for i, img_file in enumerate(sorted(img_files_direct)[:num_samples]):
                try:
                    img = Image.open(img_file)
                    images.append(img)
                except Exception as e:
                    print(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_file}, {e}")
                    continue
            
            if images:
                break
    
    if not images:
        error_msg = (
            "TID2013 ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
            "ğŸ“¥ ë‹¤ìš´ë¡œë“œ ë°©ë²•:\n"
            "1. ê³µì‹ ì›¹ì‚¬ì´íŠ¸: http://www.ponomarenko.info/tid2013.htm\n"
            "2. ë‹¤ìš´ë¡œë“œ í›„ ë‹¤ìŒ ê²½ë¡œ ì¤‘ í•˜ë‚˜ì— ë°°ì¹˜:\n"
            f"   - {os.path.join(os.getcwd(), 'data', 'TID2013', 'reference_images')}\n"
            f"   - {os.path.join(os.getcwd(), 'TID2013', 'reference_images')}\n\n"
            "ğŸ’¡ ë˜ëŠ” 'ì»¤ìŠ¤í…€ í´ë”' ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ ë³¸ì¸ì˜ ì´ë¯¸ì§€ í´ë”ë¥¼ ì§€ì •í•˜ì„¸ìš”!"
        )
        raise FileNotFoundError(error_msg)
    
    return images

def get_available_splits(dataset_name: str) -> List[str]:
    """
    Hugging Face ë°ì´í„°ì…‹ì˜ ì‚¬ìš© ê°€ëŠ¥í•œ split ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    (ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì—†ì´ ë©”íƒ€ë°ì´í„°ë§Œ í™•ì¸)
    
    Args:
        dataset_name: Hugging Face ë°ì´í„°ì…‹ ì´ë¦„
        
    Returns:
        List[str]: ì‚¬ìš© ê°€ëŠ¥í•œ split ëª©ë¡ (ì˜ˆ: ['train', 'test', 'validation'])
    """
    # ë¨¼ì € HfApië¡œ ì‹œë„ (ë‹¤ìš´ë¡œë“œ ì—†ì´ ë¹ ë¦„)
    try:
        from huggingface_hub import HfApi
        
        api = HfApi()
        dataset_info = api.dataset_info(dataset_name)
        
        # splits ì •ë³´ ì¶”ì¶œ
        if hasattr(dataset_info, 'splits') and dataset_info.splits:
            return list(dataset_info.splits.keys())
    except Exception:
        pass  # HfApi ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ë°©ë²• ì‹œë„
    
    # HfApiì—ì„œ splitsë¥¼ ëª» ê°€ì ¸ì™”ìœ¼ë©´ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ í´ë°±
    try:
        from datasets import load_dataset_builder
        
        # trust_remote_code ì—†ì´ ì‹œë„
        try:
            builder = load_dataset_builder(dataset_name)
        except Exception:
            # trust_remote_codeê°€ í•„ìš”í•œ ê²½ìš°
            builder = load_dataset_builder(dataset_name, trust_remote_code=True)
        
        # ë°©ë²• 1: builder.info.splits í™•ì¸
        if hasattr(builder, 'info') and hasattr(builder.info, 'splits'):
            splits = builder.info.splits
            if splits:
                return list(splits.keys())
        
        # ë°©ë²• 2: builder.config.data_filesì—ì„œ split ì¶”ì¶œ (Parquet ê¸°ë°˜ ë°ì´í„°ì…‹)
        if hasattr(builder, 'config') and hasattr(builder.config, 'data_files'):
            data_files = builder.config.data_files
            if data_files and isinstance(data_files, dict):
                splits = list(data_files.keys())
                if splits:
                    return splits
        
        # ë°©ë²• 3: builder.configsì—ì„œ split ì¶”ì¶œ
        if hasattr(builder, 'configs') and builder.configs:
            # ì²« ë²ˆì§¸ configì˜ data_files í™•ì¸
            first_config = builder.configs[0]
            if hasattr(first_config, 'data_files') and first_config.data_files:
                if isinstance(first_config.data_files, dict):
                    splits = list(first_config.data_files.keys())
                    if splits:
                        return splits
    except Exception:
        pass
    
    # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
    return ["train", "test", "validation", "val"]

def load_huggingface_dataset(dataset_name: str, num_samples: int = 100, split: str = "train", image_column: str = None, download_full: bool = False, download_percentage: int = None):
    """
    Hugging Face Datasetsì—ì„œ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        dataset_name: Hugging Face ë°ì´í„°ì…‹ ì´ë¦„ (ì˜ˆ: "beans", "food101", "dataset:config")
                     ':' êµ¬ë¶„ìë¡œ configë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        num_samples: ë¡œë“œí•  ìƒ˜í”Œ ê°œìˆ˜ (download_percentageê°€ Noneì¼ ë•Œ ì‚¬ìš©)
        split: ë°ì´í„°ì…‹ split (ì˜ˆ: "train", "test", "train[:100]")
        image_column: ì´ë¯¸ì§€ ì»¬ëŸ¼ ì´ë¦„ (Noneì´ë©´ ìë™ ê°ì§€)
        download_full: Trueë©´ ì „ì²´ ë‹¤ìš´ë¡œë“œ, Falseë©´ ì¼ë¶€ë§Œ (ê¸°ë³¸ê°’: False)
        download_percentage: ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ì…‹ ë¹„ìœ¨ (1-100, Noneì´ë©´ num_samples ì‚¬ìš©)
        
    Returns:
        List[PIL.Image]: ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
    """
    try:
        from datasets import load_dataset, get_dataset_config_names
        
        # ë°ì´í„°ì…‹ ì´ë¦„ì—ì„œ config ì¶”ì¶œ (dataset_name:config í˜•ì‹ ì§€ì›)
        config_name = None
        base_dataset_name = dataset_name
        if ':' in dataset_name:
            parts = dataset_name.split(':', 1)
            base_dataset_name = parts[0]
            config_name = parts[1]
        
        # í¼ì„¼í‹°ì§€ ê¸°ë°˜ ë‹¤ìš´ë¡œë“œ
        if download_percentage is not None:
            # í¼ì„¼í‹°ì§€ í˜•ì‹ìœ¼ë¡œ split ìƒì„±: train[:10%]
            actual_split = f"{split}[:{download_percentage}%]"
            num_samples_to_use = None  # í¼ì„¼í‹°ì§€ ì‚¬ìš© ì‹œ num_samples ë¬´ì‹œ
        # ìƒ˜í”Œ ê°œìˆ˜ ê¸°ë°˜ ë‹¤ìš´ë¡œë“œ
        elif not download_full:
            # splitì— ì´ë¯¸ ìŠ¬ë¼ì´ì‹±ì´ ìˆëŠ”ì§€ í™•ì¸
            if "[" not in split:
                # ìŠ¬ë¼ì´ì‹± ì¶”ê°€: train[:num_samples] í˜•ì‹
                actual_split = f"{split}[:{num_samples}]"
            else:
                actual_split = split
            num_samples_to_use = num_samples
        # ì „ì²´ ë‹¤ìš´ë¡œë“œ
        else:
            actual_split = split
            num_samples_to_use = num_samples
        
        # ë°ì´í„°ì…‹ ë¡œë“œ (configê°€ í•„ìš”í•œ ê²½ìš° ìë™ ì²˜ë¦¬)
        def try_load_dataset(dataset_name_to_use, config_to_use=None):
            """ë°ì´í„°ì…‹ ë¡œë“œë¥¼ ì‹œë„í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
            if download_full:
                # ì „ì²´ ë‹¤ìš´ë¡œë“œ: split ê·¸ëŒ€ë¡œ ì‚¬ìš© (ìŠ¬ë¼ì´ì‹± ì—†ìŒ)
                try:
                    if config_to_use:
                        return load_dataset(dataset_name_to_use, name=config_to_use, split=split, trust_remote_code=True)
                    else:
                        return load_dataset(dataset_name_to_use, split=split, trust_remote_code=True)
                except Exception:
                    if config_to_use:
                        return load_dataset(dataset_name_to_use, name=config_to_use, split=split)
                    else:
                        return load_dataset(dataset_name_to_use, split=split)
            else:
                # ì¼ë¶€ë§Œ ë‹¤ìš´ë¡œë“œ: streaming ëª¨ë“œ ì‚¬ìš© (ì „ì²´ ë‹¤ìš´ë¡œë“œ ì•ˆ í•¨)
                try:
                    # Streaming ëª¨ë“œë¡œ ì‹œë„ (ì „ì²´ ë‹¤ìš´ë¡œë“œ ì•ˆ í•¨, í•„ìš”í•œ ë¶€ë¶„ë§Œ)
                    if config_to_use:
                        return load_dataset(
                            dataset_name_to_use, 
                            name=config_to_use,
                            split=actual_split, 
                            trust_remote_code=True,
                            streaming=True
                        )
                    else:
                        return load_dataset(
                            dataset_name_to_use, 
                            split=actual_split, 
                            trust_remote_code=True,
                            streaming=True
                        )
                except Exception:
                    # streaming ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ëª¨ë“œë¡œ ì¬ì‹œë„ (ìºì‹œëœ ë°ì´í„° ì‚¬ìš©)
                    try:
                        if config_to_use:
                            return load_dataset(
                                dataset_name_to_use, 
                                name=config_to_use,
                                split=actual_split, 
                                trust_remote_code=True,
                                streaming=False
                            )
                        else:
                            return load_dataset(
                                dataset_name_to_use, 
                                split=actual_split, 
                                trust_remote_code=True,
                                streaming=False
                            )
                    except Exception:
                        if config_to_use:
                            return load_dataset(
                                dataset_name_to_use, 
                                name=config_to_use,
                                split=actual_split, 
                                trust_remote_code=True
                            )
                        else:
                            return load_dataset(
                                dataset_name_to_use, 
                                split=actual_split, 
                                trust_remote_code=True
                            )
        
        # ë°ì´í„°ì…‹ ë¡œë“œ ì‹œë„
        try:
            dataset = try_load_dataset(base_dataset_name, config_name)
        except Exception as e:
            error_msg = str(e)
            # Config ê´€ë ¨ ì—ëŸ¬ì¸ì§€ í™•ì¸
            if "Config name is missing" in error_msg or "pick one among the available configs" in error_msg:
                # ì‚¬ìš© ê°€ëŠ¥í•œ config ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                try:
                    available_configs = get_dataset_config_names(base_dataset_name)
                    if available_configs and len(available_configs) > 0:
                        # ì²« ë²ˆì§¸ config ìë™ ì‚¬ìš©
                        auto_config = available_configs[0]
                        print(f"âš ï¸ Configê°€ ì§€ì •ë˜ì§€ ì•Šì•„ ì²« ë²ˆì§¸ config '{auto_config}'ë¥¼ ìë™ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ configs: {', '.join(available_configs)}")
                        print(f"   ë‹¤ë¥¸ configë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë°ì´í„°ì…‹ ì´ë¦„ì„ '{base_dataset_name}:{auto_config}' í˜•ì‹ìœ¼ë¡œ ì§€ì •í•˜ì„¸ìš”.")
                        dataset = try_load_dataset(base_dataset_name, auto_config)
                    else:
                        raise Exception(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {base_dataset_name}\nì—ëŸ¬: {e}")
                except Exception as config_error:
                    raise Exception(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {base_dataset_name}\nì—ëŸ¬: {e}\nConfig ì¡°íšŒ ì‹¤íŒ¨: {config_error}")
            else:
                raise Exception(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {base_dataset_name}\nì—ëŸ¬: {e}")
        
        # ì´ë¯¸ì§€ ì»¬ëŸ¼ ìë™ ê°ì§€
        if image_column is None:
            # ì¼ë°˜ì ì¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ ì´ë¦„ë“¤
            possible_columns = ['image', 'images', 'img', 'photo', 'picture', 'Image', 'ImagePath']
            image_column = None
            
            for col in possible_columns:
                if col in dataset.column_names:
                    image_column = col
                    break
            
            # ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì»¬ëŸ¼ í™•ì¸
            if image_column is None:
                for col in dataset.column_names:
                    try:
                        sample = dataset[0][col]
                        if isinstance(sample, Image.Image) or hasattr(sample, 'mode'):
                            image_column = col
                            break
                    except Exception:
                        continue
        
        # ì´ë¯¸ì§€ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
        if image_column is None:
            # í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì¸ì§€ í™•ì¸
            text_indicators = ['text', 'sentence', 'premise', 'hypothesis', 'question', 'context', 'answer', 'review', 'content']
            has_text_columns = any(col.lower() in text_indicators for col in dataset.column_names)
            
            if has_text_columns:
                raise ValueError(
                    f"ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì´ ì•„ë‹Œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n"
                    f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {dataset.column_names}\n\n"
                    f"ğŸ’¡ í•´ê²° ë°©ë²•:\n"
                    f"  1. ë°ì´í„° íƒ€ì…ì„ 'í…ìŠ¤íŠ¸'ë¡œ ë³€ê²½í•˜ê±°ë‚˜\n"
                    f"  2. ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì„¸ìš”."
                )
            else:
                raise ValueError(
                    f"ì´ë¯¸ì§€ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                    f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {dataset.column_names}\n\n"
                    f"ğŸ’¡ ì´ ë°ì´í„°ì…‹ì€ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
                    f"   ë°ì´í„° íƒ€ì…ì„ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì„ ì„ íƒí•´ë³´ì„¸ìš”."
                )
        
        images = []
        # ì‚¬ìš©í•  ìƒ˜í”Œ ê°œìˆ˜ ê²°ì •
        # Streaming ëª¨ë“œì¸ ê²½ìš° len() ê³„ì‚°ì´ ëŠë¦¬ë¯€ë¡œ ì œí•œ ì‚¬ìš©
        is_streaming = hasattr(dataset, '__iter__') and not hasattr(dataset, '__len__')
        
        if is_streaming:
            # Streaming ëª¨ë“œ: í•„ìš”í•œ ê°œìˆ˜ë§Œ ìˆœíšŒ
            max_samples = num_samples_to_use if num_samples_to_use is not None else 100
            for i, item in enumerate(dataset):
                if i >= max_samples:
                    break
                try:
                    img = item[image_column]
                    
                    # PIL Imageë¡œ ë³€í™˜
                    if isinstance(img, Image.Image):
                        images.append(img)
                    elif hasattr(img, 'convert'):
                        images.append(img)
                    else:
                        import numpy as np
                        if isinstance(img, np.ndarray):
                            img_pil = Image.fromarray(img)
                            images.append(img_pil)
                        else:
                            print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {type(img)}")
                            continue
                except Exception as e:
                    print(f"ì´ë¯¸ì§€ {i} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
        else:
            # ì¼ë°˜ ëª¨ë“œ: len() ì‚¬ìš© ê°€ëŠ¥
            max_samples = num_samples_to_use if num_samples_to_use is not None else len(dataset)
            for i in range(min(max_samples, len(dataset))):
                try:
                    img = dataset[i][image_column]
                    
                    # PIL Imageë¡œ ë³€í™˜
                    if isinstance(img, Image.Image):
                        images.append(img)
                    elif hasattr(img, 'convert'):  # Image ê°ì²´ì¸ë° íƒ€ì… ì²´í¬ê°€ ì•ˆ ë˜ëŠ” ê²½ìš°
                        images.append(img)
                    else:
                        # numpy arrayë‚˜ ë‹¤ë¥¸ í˜•ì‹ì¸ ê²½ìš°
                        import numpy as np
                        if isinstance(img, np.ndarray):
                            img_pil = Image.fromarray(img)
                            images.append(img_pil)
                        else:
                            print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {type(img)}")
                            continue
                except Exception as e:
                    print(f"ì´ë¯¸ì§€ {i} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
        
        return images
    
    except ImportError:
        raise ImportError("datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install datasets ì‹¤í–‰í•˜ì„¸ìš”.")
    except Exception as e:
        raise Exception(f"Hugging Face ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨ ({dataset_name}): {e}")

def load_huggingface_text_dataset(dataset_name: str, num_samples: int = 100, split: str = "train", text_column: str = None, download_percentage: int = None, download_full: bool = False):
    """
    Hugging Face Datasetsì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        dataset_name: Hugging Face ë°ì´í„°ì…‹ ì´ë¦„ (ì˜ˆ: "imdb", "yelp_review_full", "nyu-mll/glue:sst2")
                     ':' êµ¬ë¶„ìë¡œ configë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì˜ˆ: "nyu-mll/glue:sst2")
        num_samples: ë¡œë“œí•  ìƒ˜í”Œ ê°œìˆ˜ (download_percentageê°€ Noneì¼ ë•Œ ì‚¬ìš©)
        split: ë°ì´í„°ì…‹ split (ì˜ˆ: "train", "test")
        text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„ (Noneì´ë©´ ìë™ ê°ì§€)
        download_percentage: ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ì…‹ ë¹„ìœ¨ (1-100, Noneì´ë©´ num_samples ì‚¬ìš©)
        download_full: Trueë©´ ì „ì²´ ë‹¤ìš´ë¡œë“œ
        
    Returns:
        List[str]: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    # get_available_splits í•¨ìˆ˜ëŠ” ì´ë¯¸ ìœ„ì— ì •ì˜ë˜ì–´ ìˆìŒ
    try:
        from datasets import load_dataset, get_dataset_config_names
        
        # ë°ì´í„°ì…‹ ì´ë¦„ì—ì„œ config ì¶”ì¶œ (dataset_name:config í˜•ì‹ ì§€ì›)
        config_name = None
        base_dataset_name = dataset_name
        if ':' in dataset_name:
            parts = dataset_name.split(':', 1)
            base_dataset_name = parts[0]
            config_name = parts[1]
        
        # í¼ì„¼í‹°ì§€ ê¸°ë°˜ ë‹¤ìš´ë¡œë“œ
        if download_percentage is not None:
            actual_split = f"{split}[:{download_percentage}%]"
            num_samples_to_use = None
        elif not download_full:
            if "[" not in split:
                actual_split = f"{split}[:{num_samples}]"
            else:
                actual_split = split
            num_samples_to_use = num_samples
        else:
            actual_split = split
            num_samples_to_use = None
        
        # ë°ì´í„°ì…‹ ë¡œë“œ (configê°€ í•„ìš”í•œ ê²½ìš° ìë™ ì²˜ë¦¬)
        def try_load_dataset(dataset_name_to_use, config_to_use=None, split_to_use=None):
            """ë°ì´í„°ì…‹ ë¡œë“œë¥¼ ì‹œë„í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
            if split_to_use is None:
                split_to_use = split if download_full else actual_split
            
            if download_full:
                # ì „ì²´ ë‹¤ìš´ë¡œë“œ
                try:
                    if config_to_use:
                        return load_dataset(dataset_name_to_use, name=config_to_use, split=split_to_use, trust_remote_code=True)
                    else:
                        return load_dataset(dataset_name_to_use, split=split_to_use, trust_remote_code=True)
                except Exception:
                    if config_to_use:
                        return load_dataset(dataset_name_to_use, name=config_to_use, split=split_to_use)
                    else:
                        return load_dataset(dataset_name_to_use, split=split_to_use)
            else:
                # ì¼ë¶€ë§Œ ë‹¤ìš´ë¡œë“œ: streaming ëª¨ë“œ ì‚¬ìš©
                try:
                    if config_to_use:
                        return load_dataset(
                            dataset_name_to_use, 
                            name=config_to_use,
                            split=split_to_use, 
                            trust_remote_code=True,
                            streaming=True
                        )
                    else:
                        return load_dataset(
                            dataset_name_to_use, 
                            split=split_to_use, 
                            trust_remote_code=True,
                            streaming=True
                        )
                except Exception:
                    # streaming ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ëª¨ë“œ
                    if config_to_use:
                        return load_dataset(
                            dataset_name_to_use, 
                            name=config_to_use,
                            split=split_to_use, 
                            trust_remote_code=True
                        )
                    else:
                        return load_dataset(
                            dataset_name_to_use, 
                            split=split_to_use, 
                            trust_remote_code=True
                        )
        
        # ë°ì´í„°ì…‹ ë¡œë“œ ì‹œë„ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        dataset = None
        current_config = config_name
        current_split = split
        current_actual_split = actual_split
        max_retries = 3
        retry_count = 0
        
        while dataset is None and retry_count < max_retries:
            try:
                split_to_use = current_split if download_full else current_actual_split
                dataset = try_load_dataset(base_dataset_name, current_config, split_to_use)
            except Exception as e:
                error_msg = str(e)
                retry_count += 1
                
                # Config ê´€ë ¨ ì—ëŸ¬ì¸ì§€ í™•ì¸
                if "Config name is missing" in error_msg or "pick one among the available configs" in error_msg:
                    # ì‚¬ìš© ê°€ëŠ¥í•œ config ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                    try:
                        from datasets import load_dataset_builder
                        available_configs = get_dataset_config_names(base_dataset_name)
                        
                        if available_configs and len(available_configs) > 0:
                            # train splitì´ ìˆëŠ” configë¥¼ ìš°ì„  ì°¾ê¸°
                            preferred_config = None
                            for config in available_configs:
                                try:
                                    builder = load_dataset_builder(base_dataset_name, config_name=config)
                                    if hasattr(builder, 'info') and hasattr(builder.info, 'splits'):
                                        splits = builder.info.splits
                                        if splits and 'train' in splits:
                                            preferred_config = config
                                            break
                                except Exception:
                                    continue
                            
                            # train splitì´ ìˆëŠ” configê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ config ì‚¬ìš©
                            auto_config = preferred_config if preferred_config else available_configs[0]
                            current_config = auto_config
                            
                            print(f"âš ï¸ Configê°€ ì§€ì •ë˜ì§€ ì•Šì•„ config '{auto_config}'ë¥¼ ìë™ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                            print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ configs: {', '.join(available_configs)}")
                            print(f"   ë‹¤ë¥¸ configë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë°ì´í„°ì…‹ ì´ë¦„ì„ '{base_dataset_name}:{auto_config}' í˜•ì‹ìœ¼ë¡œ ì§€ì •í•˜ì„¸ìš”.")
                            
                            # ì„ íƒëœ configì— ì‚¬ìš© ê°€ëŠ¥í•œ split í™•ì¸ ë° ìë™ ì¡°ì •
                            try:
                                builder = load_dataset_builder(base_dataset_name, config_name=auto_config)
                                if hasattr(builder, 'info') and hasattr(builder.info, 'splits'):
                                    available_splits = list(builder.info.splits.keys())
                                    if current_split not in available_splits:
                                        # train -> test -> validation ìˆœì„œë¡œ ìš°ì„ ìˆœìœ„
                                        for preferred_split in ['train', 'test', 'validation', 'val']:
                                            if preferred_split in available_splits:
                                                current_split = preferred_split
                                                # actual_splitë„ ì—…ë°ì´íŠ¸
                                                if "[" not in current_split:
                                                    current_actual_split = f"{current_split}[:{num_samples}]"
                                                else:
                                                    current_actual_split = current_split
                                                print(f"âš ï¸ ìš”ì²­í•œ splitì´ ì—†ì–´ '{current_split}' splitì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                                                print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ splits: {', '.join(available_splits)}")
                                                break
                            except Exception:
                                pass  # split í™•ì¸ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                            
                            # ì¬ì‹œë„
                            retry_count -= 1  # ì¬ì‹œë„ ì¹´ìš´íŠ¸ ì¡°ì • (ì´ë²ˆ ì‹œë„ëŠ” ì¬ì‹œë„ë¡œ ê°„ì£¼í•˜ì§€ ì•ŠìŒ)
                            continue
                        else:
                            raise Exception(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {base_dataset_name}\nì—ëŸ¬: {e}")
                    except Exception as config_error:
                        raise Exception(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {base_dataset_name}\nì—ëŸ¬: {e}\nConfig ì¡°íšŒ ì‹¤íŒ¨: {config_error}")
                
                # Split ê´€ë ¨ ì—ëŸ¬ì¸ì§€ í™•ì¸
                elif "Unknown split" in error_msg or "Should be one of" in error_msg:
                    # ì‚¬ìš© ê°€ëŠ¥í•œ split ì°¾ê¸°
                    try:
                        from datasets import load_dataset_builder
                        builder = load_dataset_builder(base_dataset_name, config_name=current_config) if current_config else load_dataset_builder(base_dataset_name)
                        
                        if hasattr(builder, 'info') and hasattr(builder.info, 'splits'):
                            available_splits = list(builder.info.splits.keys())
                            # train -> test -> validation ìˆœì„œë¡œ ìš°ì„ ìˆœìœ„
                            for preferred_split in ['train', 'test', 'validation', 'val']:
                                if preferred_split in available_splits:
                                    current_split = preferred_split
                                    # actual_splitë„ ì—…ë°ì´íŠ¸
                                    if "[" not in current_split:
                                        current_actual_split = f"{current_split}[:{num_samples}]"
                                    else:
                                        current_actual_split = current_split
                                    print(f"âš ï¸ ìš”ì²­í•œ splitì´ ì—†ì–´ '{current_split}' splitì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                                    print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ splits: {', '.join(available_splits)}")
                                    break
                            else:
                                current_split = available_splits[0] if available_splits else "test"
                                if "[" not in current_split:
                                    current_actual_split = f"{current_split}[:{num_samples}]"
                                else:
                                    current_actual_split = current_split
                                print(f"âš ï¸ split '{current_split}'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                                print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ splits: {', '.join(available_splits)}")
                            
                            # ì¬ì‹œë„
                            retry_count -= 1  # ì¬ì‹œë„ ì¹´ìš´íŠ¸ ì¡°ì • (ì´ë²ˆ ì‹œë„ëŠ” ì¬ì‹œë„ë¡œ ê°„ì£¼í•˜ì§€ ì•ŠìŒ)
                            continue
                        else:
                            raise Exception(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {base_dataset_name}\nì—ëŸ¬: {e}")
                    except Exception as split_error:
                        raise Exception(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {base_dataset_name}\nì—ëŸ¬: {e}\nSplit ì¡°íšŒ ì‹¤íŒ¨: {split_error}")
                else:
                    if retry_count >= max_retries:
                        raise Exception(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {base_dataset_name}\nì—ëŸ¬: {e}")
                    else:
                        raise Exception(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {base_dataset_name}\nì—ëŸ¬: {e}")
        
        if dataset is None:
            raise Exception(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {base_dataset_name} (ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼)")
        
        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ìë™ ê°ì§€
        if text_column is None:
            possible_columns = ['text', 'Text', 'review', 'content', 'sentence', 'document', 'abstract', 'body']
            text_column = None
            
            for col in possible_columns:
                if col in dataset.column_names:
                    text_column = col
                    break
            
            # ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë¬¸ìì—´ ì»¬ëŸ¼ ì°¾ê¸°
            if text_column is None:
                for col in dataset.column_names:
                    if col not in ['label', 'labels', 'id', 'idx']:
                        try:
                            sample = dataset[0][col]
                            if isinstance(sample, str):
                                text_column = col
                                break
                        except:
                            continue
        
        if text_column is None:
            raise ValueError(f"í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {dataset.column_names}")
        
        texts = []
        # Streaming ëª¨ë“œì¸ ê²½ìš° ì²˜ë¦¬
        is_streaming = hasattr(dataset, '__iter__') and not hasattr(dataset, '__len__')
        
        if is_streaming:
            # Streaming ëª¨ë“œ: í•„ìš”í•œ ê°œìˆ˜ë§Œ ìˆœíšŒ
            max_samples = num_samples_to_use if num_samples_to_use is not None else 100
            for i, item in enumerate(dataset):
                if i >= max_samples:
                    break
                try:
                    text = item[text_column]
                    if isinstance(text, str) and len(text.strip()) > 0:
                        texts.append(text)
                except Exception as e:
                    print(f"í…ìŠ¤íŠ¸ {i} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
        else:
            # ì¼ë°˜ ëª¨ë“œ: len() ì‚¬ìš© ê°€ëŠ¥
            max_samples = num_samples_to_use if num_samples_to_use is not None else len(dataset)
            for i in range(min(max_samples, len(dataset))):
                try:
                    text = dataset[i][text_column]
                    if isinstance(text, str) and len(text.strip()) > 0:
                        texts.append(text)
                except Exception as e:
                    print(f"í…ìŠ¤íŠ¸ {i} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
        
        return texts
    
    except ImportError:
        raise ImportError("datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install datasets ì‹¤í–‰í•˜ì„¸ìš”.")
    except Exception as e:
        raise Exception(f"Hugging Face í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨ ({dataset_name}): {e}")

def load_custom_dataset(folder_path: str, num_samples: int = 100):
    """
    ë¡œì»¬ í´ë”ì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        folder_path: ì´ë¯¸ì§€ê°€ ìˆëŠ” í´ë” ê²½ë¡œ
        num_samples: ìµœëŒ€ ë¡œë“œí•  ì´ë¯¸ì§€ ê°œìˆ˜
        
    Returns:
        List[PIL.Image]: ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
    """
    import os
    import glob
    
    if not os.path.exists(folder_path):
        raise FileNotFoundError(f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
    
    # ì§€ì›í•˜ëŠ” ì´ë¯¸ì§€ í™•ì¥ì
    extensions = ["*.jpg", "*.jpeg", "*.png", "*.bmp", "*.gif"]
    
    image_files = []
    for ext in extensions:
        image_files.extend(glob.glob(os.path.join(folder_path, ext)))
        image_files.extend(glob.glob(os.path.join(folder_path, ext.upper())))
    
    if not image_files:
        raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
    
    images = []
    for img_file in image_files[:num_samples]:
        try:
            img = Image.open(img_file)
            images.append(img)
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_file}, {e}")
            continue
    
    return images

