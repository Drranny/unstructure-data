"""
ë°ì´í„°ì…‹ ë°°ì¹˜ ë¶„ì„ ëª¨ë“ˆ
CIFAR-10, TID2013 ë“±ì˜ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë°°ì¹˜ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
"""
import numpy as np
from PIL import Image
import imagehash
from typing import List, Dict
from src.image_quality import analyze_image_quality, calculate_duplication_score
from src.text_quality import analyze_text_quality
from src.utils import calc_total_score

def analyze_dataset_images(images: List[Image.Image], max_samples: int = 100) -> Dict:
    """
    ì—¬ëŸ¬ ì´ë¯¸ì§€ì˜ í’ˆì§ˆì„ ë°°ì¹˜ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        images: PIL Image ê°ì²´ ë¦¬ìŠ¤íŠ¸
        max_samples: ìµœëŒ€ ë¶„ì„í•  ì´ë¯¸ì§€ ê°œìˆ˜ (ì„±ëŠ¥ ê³ ë ¤)
        
    Returns:
        dict: ì „ì²´ ë°ì´í„°ì…‹ì˜ í’ˆì§ˆ í†µê³„
    """
    if len(images) == 0:
        return {
            "ì´ ì´ë¯¸ì§€ ìˆ˜": 0,
            "í‰ê·  í•´ìƒë„": 0.0,
            "í‰ê·  ì„ ëª…ë„": 0.0,
            "í‰ê·  ë…¸ì´ì¦ˆ": 0.0,
            "í‰ê·  ì¤‘ë³µë„": 0.0,
            "í‰ê·  ì¢…í•© ì ìˆ˜": 0.0,
        }
    
    # ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´ ì¼ë¶€ë§Œ)
    # ì°¸ê³ : random.sampleì„ ì‚¬ìš©í•˜ì—¬ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ë¯€ë¡œ,
    # í•´ìƒë„ê°€ ë‹¤ë¥¸ ì´ë¯¸ì§€ë“¤ì´ ê³¨ê³ ë£¨ ì„ íƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    original_count = len(images)
    if len(images) > max_samples:
        import random
        images = random.sample(images, max_samples)
    
    all_scores = {
        "í•´ìƒë„": [],
        "ì„ ëª…ë„": [],
        "ë…¸ì´ì¦ˆ": [],
        "ì¤‘ë³µë„": [],
        "ì¢…í•©ì ìˆ˜": []
    }
    
    # ì‹¤ì œ í•´ìƒë„ ì •ë³´ ì €ì¥ (width x height)
    actual_resolutions = []  # (width, height) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
    
    image_hashes = []
    
    # ê° ì´ë¯¸ì§€ ë¶„ì„
    for img in images:
        scores = analyze_image_quality(img)
        total = calc_total_score(scores)
        
        all_scores["í•´ìƒë„"].append(scores["í•´ìƒë„"])
        all_scores["ì„ ëª…ë„"].append(scores["ì„ ëª…ë„"])
        all_scores["ë…¸ì´ì¦ˆ"].append(scores["ë…¸ì´ì¦ˆ"])
        all_scores["ì¤‘ë³µë„"].append(scores["ì¤‘ë³µë„"])
        all_scores["ì¢…í•©ì ìˆ˜"].append(total)
        
        # ì‹¤ì œ í•´ìƒë„ ì €ì¥ (width x height)
        actual_resolutions.append((img.width, img.height))
        
        # ì¤‘ë³µë„ ê³„ì‚°ì„ ìœ„í•œ í•´ì‹œ ì €ì¥
        image_hashes.append(imagehash.average_hash(img))
    
    # ì¤‘ë³µë„ ì¬ê³„ì‚° (ì „ì²´ ì´ë¯¸ì§€ ê°„)
    if len(image_hashes) > 1:
        duplication = calculate_duplication_score(image_hashes)
        # ì¤‘ë³µë„ ì ìˆ˜ ì—…ë°ì´íŠ¸
        all_scores["ì¤‘ë³µë„"] = [duplication] * len(all_scores["ì¤‘ë³µë„"])
    
    # í•´ìƒë„ í†µê³„ ê³„ì‚°
    widths = [r[0] for r in actual_resolutions]
    heights = [r[1] for r in actual_resolutions]
    total_pixels = [w * h for w, h in actual_resolutions]
    
    # í†µê³„ ê³„ì‚°
    result = {
        "ì´ ì´ë¯¸ì§€ ìˆ˜": len(images),
        "ì›ë³¸ ë°ì´í„°ì…‹ í¬ê¸°": original_count if original_count > len(images) else len(images),
        "ìƒ˜í”Œë§ ì—¬ë¶€": "ì˜ˆ" if original_count > len(images) else "ì•„ë‹ˆì˜¤",
        "í‰ê·  í•´ìƒë„": round(np.mean(all_scores["í•´ìƒë„"]), 3),
        "í‰ê·  ì„ ëª…ë„": round(np.mean(all_scores["ì„ ëª…ë„"]), 3),
        "í‰ê·  ë…¸ì´ì¦ˆ": round(np.mean(all_scores["ë…¸ì´ì¦ˆ"]), 3),
        "í‰ê·  ì¤‘ë³µë„": round(np.mean(all_scores["ì¤‘ë³µë„"]), 3),
        "í‰ê·  ì¢…í•© ì ìˆ˜": round(np.mean(all_scores["ì¢…í•©ì ìˆ˜"]), 3),
        "ìµœì†Œ ì¢…í•© ì ìˆ˜": round(np.min(all_scores["ì¢…í•©ì ìˆ˜"]), 3),
        "ìµœëŒ€ ì¢…í•© ì ìˆ˜": round(np.max(all_scores["ì¢…í•©ì ìˆ˜"]), 3),
        "í‘œì¤€í¸ì°¨": round(np.std(all_scores["ì¢…í•©ì ìˆ˜"]), 3),
        # ì‹¤ì œ í•´ìƒë„ ì •ë³´ ì¶”ê°€
        "í•´ìƒë„ ë¶„í¬": {
            "ìµœì†Œ": f"{min(widths)}x{min(heights)}",
            "ìµœëŒ€": f"{max(widths)}x{max(heights)}",
            "í‰ê· ": f"{int(np.mean(widths))}x{int(np.mean(heights))}",
            "ì¤‘ì•™ê°’": f"{int(np.median(widths))}x{int(np.median(heights))}",
            "í‰ê·  í”½ì…€ ìˆ˜": f"{int(np.mean(total_pixels)):,}",
        },
        "í•´ìƒë„ ëª©ë¡": [f"{w}x{h}" for w, h in actual_resolutions],  # ì„ íƒëœ ì´ë¯¸ì§€ë“¤ì˜ ì‹¤ì œ í•´ìƒë„
    }
    
    return result

def analyze_dataset_texts(texts: List[str], max_samples: int = 100) -> Dict:
    """
    ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ ë°°ì¹˜ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        texts: í…ìŠ¤íŠ¸ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        max_samples: ìµœëŒ€ ë¶„ì„í•  í…ìŠ¤íŠ¸ ê°œìˆ˜ (ì„±ëŠ¥ ê³ ë ¤)
        
    Returns:
        dict: ì „ì²´ ë°ì´í„°ì…‹ì˜ í’ˆì§ˆ í†µê³„
    """
    if len(texts) == 0:
        return {
            "ì´ í…ìŠ¤íŠ¸ ìˆ˜": 0,
            "í‰ê·  ì •í™•ì„±": 0.0,
            "í‰ê·  ì¤‘ë³µë„": 0.0,
            "í‰ê·  ì™„ì „ì„±": 0.0,
            "í‰ê·  ì¢…í•© ì ìˆ˜": 0.0,
        }
    
    # ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´ ì¼ë¶€ë§Œ)
    if len(texts) > max_samples:
        import random
        texts = random.sample(texts, max_samples)
    
    all_scores = {
        "ì •í™•ì„±": [],
        "ì¤‘ë³µë„": [],
        "ì™„ì „ì„±": [],
        "ì¢…í•©ì ìˆ˜": []
    }
    
    # ê° í…ìŠ¤íŠ¸ ë¶„ì„
    for text in texts:
        if not text or len(text.strip()) == 0:
            continue
            
        scores = analyze_text_quality(text)
        total = calc_total_score(scores)
        
        all_scores["ì •í™•ì„±"].append(scores["ì •í™•ì„±(ì˜¤íƒˆìë¹„ìœ¨)"])
        all_scores["ì¤‘ë³µë„"].append(scores["ì¤‘ë³µë„(ìœ ì‚¬ë„ì—­ë¹„ìœ¨)"])
        all_scores["ì™„ì „ì„±"].append(scores["ì™„ì „ì„±(ë¬¸ì¥ì¶©ì‹¤ë„)"])
        all_scores["ì¢…í•©ì ìˆ˜"].append(total)
    
    if len(all_scores["ì¢…í•©ì ìˆ˜"]) == 0:
        return {
            "ì´ í…ìŠ¤íŠ¸ ìˆ˜": 0,
            "í‰ê·  ì •í™•ì„±": 0.0,
            "í‰ê·  ì¤‘ë³µë„": 0.0,
            "í‰ê·  ì™„ì „ì„±": 0.0,
            "í‰ê·  ì¢…í•© ì ìˆ˜": 0.0,
        }
    
    # í†µê³„ ê³„ì‚°
    result = {
        "ì´ í…ìŠ¤íŠ¸ ìˆ˜": len(all_scores["ì¢…í•©ì ìˆ˜"]),
        "í‰ê·  ì •í™•ì„±": round(np.mean(all_scores["ì •í™•ì„±"]), 3),
        "í‰ê·  ì¤‘ë³µë„": round(np.mean(all_scores["ì¤‘ë³µë„"]), 3),
        "í‰ê·  ì™„ì „ì„±": round(np.mean(all_scores["ì™„ì „ì„±"]), 3),
        "í‰ê·  ì¢…í•© ì ìˆ˜": round(np.mean(all_scores["ì¢…í•©ì ìˆ˜"]), 3),
        "ìµœì†Œ ì¢…í•© ì ìˆ˜": round(np.min(all_scores["ì¢…í•©ì ìˆ˜"]), 3),
        "ìµœëŒ€ ì¢…í•© ì ìˆ˜": round(np.max(all_scores["ì¢…í•©ì ìˆ˜"]), 3),
        "í‘œì¤€í¸ì°¨": round(np.std(all_scores["ì¢…í•©ì ìˆ˜"]), 3),
    }
    
    return result

def load_cifar10(num_samples: int = 100):
    """
    CIFAR-10 ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        num_samples: ë¡œë“œí•  ìƒ˜í”Œ ê°œìˆ˜
        
    Returns:
        List[PIL.Image]: ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
    """
    try:
        from torchvision import datasets
        from torchvision import transforms
        
        # CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ
        transform = transforms.Compose([
            transforms.ToPILImage()
        ])
        
        dataset = datasets.CIFAR10(
            root='./data', 
            train=True, 
            download=True, 
            transform=None
        )
        
        images = []
        for i in range(min(num_samples, len(dataset))):
            img, _ = dataset[i]
            if isinstance(img, Image.Image):
                images.append(img)
            else:
                # numpy arrayë‚˜ tensorì¸ ê²½ìš° ë³€í™˜
                img_pil = Image.fromarray(np.array(img))
                images.append(img_pil)
        
        return images
    
    except ImportError:
        raise ImportError("torchvisionì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install torchvision ì‹¤í–‰í•˜ì„¸ìš”.")
    except Exception as e:
        raise Exception(f"CIFAR-10 ë¡œë“œ ì‹¤íŒ¨: {e}")

def load_tid2013(num_samples: int = 100, custom_path: str = None):
    """
    TID2013 ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        num_samples: ë¡œë“œí•  ìƒ˜í”Œ ê°œìˆ˜
        custom_path: ì»¤ìŠ¤í…€ ê²½ë¡œ ì§€ì • (ì„ íƒì‚¬í•­)
        
    Returns:
        List[PIL.Image]: ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
    """
    import os
    import glob
    
    # TID2013ì€ ì§ì ‘ ë‹¤ìš´ë¡œë“œê°€ ì–´ë ¤ìš°ë¯€ë¡œ ë¡œì»¬ ê²½ë¡œì—ì„œ ë¡œë“œ
    # ë˜ëŠ” ì»¤ìŠ¤í…€ ê²½ë¡œ ì‚¬ìš©
    
    # ê°€ëŠ¥í•œ ê²½ë¡œë“¤
    if custom_path:
        possible_paths = [custom_path]
    else:
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ìƒì„±
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        possible_paths = [
            os.path.join(project_root, "data", "TID2013"),
            os.path.join(project_root, "TID2013"),
            os.path.join(".", "data", "TID2013"),
            os.path.join(".", "TID2013"),
            os.path.join("/", "data", "TID2013"),
            os.path.expanduser("~/TID2013"),
            os.path.expanduser("~/data/TID2013"),
        ]
    
    images = []
    
    for base_path in possible_paths:
        if not os.path.exists(base_path):
            continue
            
        # TID2013 êµ¬ì¡° í™•ì¸
        # 1. reference_images í´ë” ì•ˆì˜ ì´ë¯¸ì§€
        ref_path = os.path.join(base_path, "reference_images")
        if os.path.exists(ref_path):
            img_files = []
            img_files.extend(glob.glob(os.path.join(ref_path, "*.bmp")))
            img_files.extend(glob.glob(os.path.join(ref_path, "*.png")))
            img_files.extend(glob.glob(os.path.join(ref_path, "*.jpg")))
            img_files.extend(glob.glob(os.path.join(ref_path, "*.jpeg")))
            
            if img_files:
                for i, img_file in enumerate(sorted(img_files)[:num_samples]):
                    try:
                        img = Image.open(img_file)
                        images.append(img)
                    except Exception as e:
                        print(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_file}, {e}")
                        continue
                
                if images:
                    break
        
        # 2. ì§ì ‘ ì´ë¯¸ì§€ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°
        img_files_direct = []
        img_files_direct.extend(glob.glob(os.path.join(base_path, "*.bmp")))
        img_files_direct.extend(glob.glob(os.path.join(base_path, "*.png")))
        img_files_direct.extend(glob.glob(os.path.join(base_path, "*.jpg")))
        img_files_direct.extend(glob.glob(os.path.join(base_path, "*.jpeg")))
        
        if img_files_direct:
            for i, img_file in enumerate(sorted(img_files_direct)[:num_samples]):
                try:
                    img = Image.open(img_file)
                    images.append(img)
                except Exception as e:
                    print(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_file}, {e}")
                    continue
            
            if images:
                break
    
    if not images:
        error_msg = (
            "TID2013 ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
            "ğŸ“¥ ë‹¤ìš´ë¡œë“œ ë°©ë²•:\n"
            "1. ê³µì‹ ì›¹ì‚¬ì´íŠ¸: http://www.ponomarenko.info/tid2013.htm\n"
            "2. ë‹¤ìš´ë¡œë“œ í›„ ë‹¤ìŒ ê²½ë¡œ ì¤‘ í•˜ë‚˜ì— ë°°ì¹˜:\n"
            f"   - {os.path.join(os.getcwd(), 'data', 'TID2013', 'reference_images')}\n"
            f"   - {os.path.join(os.getcwd(), 'TID2013', 'reference_images')}\n\n"
            "ğŸ’¡ ë˜ëŠ” 'ì»¤ìŠ¤í…€ í´ë”' ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ ë³¸ì¸ì˜ ì´ë¯¸ì§€ í´ë”ë¥¼ ì§€ì •í•˜ì„¸ìš”!"
        )
        raise FileNotFoundError(error_msg)
    
    return images

def load_huggingface_dataset(dataset_name: str, num_samples: int = 100, split: str = "train", image_column: str = None, download_full: bool = False, download_percentage: int = None):
    """
    Hugging Face Datasetsì—ì„œ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        dataset_name: Hugging Face ë°ì´í„°ì…‹ ì´ë¦„ (ì˜ˆ: "beans", "food101")
        num_samples: ë¡œë“œí•  ìƒ˜í”Œ ê°œìˆ˜ (download_percentageê°€ Noneì¼ ë•Œ ì‚¬ìš©)
        split: ë°ì´í„°ì…‹ split (ì˜ˆ: "train", "test", "train[:100]")
        image_column: ì´ë¯¸ì§€ ì»¬ëŸ¼ ì´ë¦„ (Noneì´ë©´ ìë™ ê°ì§€)
        download_full: Trueë©´ ì „ì²´ ë‹¤ìš´ë¡œë“œ, Falseë©´ ì¼ë¶€ë§Œ (ê¸°ë³¸ê°’: False)
        download_percentage: ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ì…‹ ë¹„ìœ¨ (1-100, Noneì´ë©´ num_samples ì‚¬ìš©)
        
    Returns:
        List[PIL.Image]: ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
    """
    try:
        from datasets import load_dataset
        
        # í¼ì„¼í‹°ì§€ ê¸°ë°˜ ë‹¤ìš´ë¡œë“œ
        if download_percentage is not None:
            # í¼ì„¼í‹°ì§€ í˜•ì‹ìœ¼ë¡œ split ìƒì„±: train[:10%]
            actual_split = f"{split}[:{download_percentage}%]"
            num_samples_to_use = None  # í¼ì„¼í‹°ì§€ ì‚¬ìš© ì‹œ num_samples ë¬´ì‹œ
        # ìƒ˜í”Œ ê°œìˆ˜ ê¸°ë°˜ ë‹¤ìš´ë¡œë“œ
        elif not download_full:
            # splitì— ì´ë¯¸ ìŠ¬ë¼ì´ì‹±ì´ ìˆëŠ”ì§€ í™•ì¸
            if "[" not in split:
                # ìŠ¬ë¼ì´ì‹± ì¶”ê°€: train[:num_samples] í˜•ì‹
                actual_split = f"{split}[:{num_samples}]"
            else:
                actual_split = split
            num_samples_to_use = num_samples
        # ì „ì²´ ë‹¤ìš´ë¡œë“œ
        else:
            actual_split = split
            num_samples_to_use = num_samples
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        try:
            if download_full:
                # ì „ì²´ ë‹¤ìš´ë¡œë“œ: split ê·¸ëŒ€ë¡œ ì‚¬ìš© (ìŠ¬ë¼ì´ì‹± ì—†ìŒ)
                dataset = load_dataset(dataset_name, split=split, trust_remote_code=True)
            else:
                # ì¼ë¶€ë§Œ ë‹¤ìš´ë¡œë“œ: streaming ëª¨ë“œ ì‚¬ìš© (ì „ì²´ ë‹¤ìš´ë¡œë“œ ì•ˆ í•¨)
                try:
                    # Streaming ëª¨ë“œë¡œ ì‹œë„ (ì „ì²´ ë‹¤ìš´ë¡œë“œ ì•ˆ í•¨, í•„ìš”í•œ ë¶€ë¶„ë§Œ)
                    dataset = load_dataset(
                        dataset_name, 
                        split=actual_split, 
                        trust_remote_code=True,
                        streaming=True  # í•„ìš”í•œ ë¶€ë¶„ë§Œ ìŠ¤íŠ¸ë¦¬ë°, ì „ì²´ ë‹¤ìš´ë¡œë“œ ì•ˆ í•¨
                    )
                except Exception:
                    # streaming ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ëª¨ë“œë¡œ ì¬ì‹œë„ (ìºì‹œëœ ë°ì´í„° ì‚¬ìš©)
                    try:
                        dataset = load_dataset(
                            dataset_name, 
                            split=actual_split, 
                            trust_remote_code=True,
                            streaming=False
                        )
                    except Exception:
                        dataset = load_dataset(
                            dataset_name, 
                            split=actual_split, 
                            trust_remote_code=True
                        )
        except Exception as e:
            # trust_remote_code ì˜¤ë¥˜ ì‹œ ì¬ì‹œë„
            try:
                if download_full:
                    dataset = load_dataset(dataset_name, split=split)
                else:
                    dataset = load_dataset(dataset_name, split=actual_split)
            except Exception:
                raise Exception(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {dataset_name}\nì—ëŸ¬: {e}")
        
        # ì´ë¯¸ì§€ ì»¬ëŸ¼ ìë™ ê°ì§€
        if image_column is None:
            # ì¼ë°˜ì ì¸ ì´ë¯¸ì§€ ì»¬ëŸ¼ ì´ë¦„ë“¤
            possible_columns = ['image', 'images', 'img', 'photo', 'picture', 'Image', 'ImagePath']
            image_column = None
            
            for col in possible_columns:
                if col in dataset.column_names:
                    image_column = col
                    break
            
            # ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì»¬ëŸ¼ í™•ì¸
            if image_column is None:
                for col in dataset.column_names:
                    sample = dataset[0][col]
                    if isinstance(sample, Image.Image) or hasattr(sample, 'mode'):
                        image_column = col
                        break
        
        if image_column is None:
            raise ValueError(f"ì´ë¯¸ì§€ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {dataset.column_names}")
        
        images = []
        # ì‚¬ìš©í•  ìƒ˜í”Œ ê°œìˆ˜ ê²°ì •
        # Streaming ëª¨ë“œì¸ ê²½ìš° len() ê³„ì‚°ì´ ëŠë¦¬ë¯€ë¡œ ì œí•œ ì‚¬ìš©
        is_streaming = hasattr(dataset, '__iter__') and not hasattr(dataset, '__len__')
        
        if is_streaming:
            # Streaming ëª¨ë“œ: í•„ìš”í•œ ê°œìˆ˜ë§Œ ìˆœíšŒ
            max_samples = num_samples_to_use if num_samples_to_use is not None else 100
            for i, item in enumerate(dataset):
                if i >= max_samples:
                    break
                try:
                    img = item[image_column]
                    
                    # PIL Imageë¡œ ë³€í™˜
                    if isinstance(img, Image.Image):
                        images.append(img)
                    elif hasattr(img, 'convert'):
                        images.append(img)
                    else:
                        import numpy as np
                        if isinstance(img, np.ndarray):
                            img_pil = Image.fromarray(img)
                            images.append(img_pil)
                        else:
                            print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {type(img)}")
                            continue
                except Exception as e:
                    print(f"ì´ë¯¸ì§€ {i} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
        else:
            # ì¼ë°˜ ëª¨ë“œ: len() ì‚¬ìš© ê°€ëŠ¥
            max_samples = num_samples_to_use if num_samples_to_use is not None else len(dataset)
            for i in range(min(max_samples, len(dataset))):
                try:
                    img = dataset[i][image_column]
                    
                    # PIL Imageë¡œ ë³€í™˜
                    if isinstance(img, Image.Image):
                        images.append(img)
                    elif hasattr(img, 'convert'):  # Image ê°ì²´ì¸ë° íƒ€ì… ì²´í¬ê°€ ì•ˆ ë˜ëŠ” ê²½ìš°
                        images.append(img)
                    else:
                        # numpy arrayë‚˜ ë‹¤ë¥¸ í˜•ì‹ì¸ ê²½ìš°
                        import numpy as np
                        if isinstance(img, np.ndarray):
                            img_pil = Image.fromarray(img)
                            images.append(img_pil)
                        else:
                            print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {type(img)}")
                            continue
                except Exception as e:
                    print(f"ì´ë¯¸ì§€ {i} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
        
        return images
    
    except ImportError:
        raise ImportError("datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install datasets ì‹¤í–‰í•˜ì„¸ìš”.")
    except Exception as e:
        raise Exception(f"Hugging Face ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨ ({dataset_name}): {e}")

def load_huggingface_text_dataset(dataset_name: str, num_samples: int = 100, split: str = "train", text_column: str = None, download_percentage: int = None, download_full: bool = False):
    """
    Hugging Face Datasetsì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        dataset_name: Hugging Face ë°ì´í„°ì…‹ ì´ë¦„ (ì˜ˆ: "imdb", "yelp_review_full")
        num_samples: ë¡œë“œí•  ìƒ˜í”Œ ê°œìˆ˜ (download_percentageê°€ Noneì¼ ë•Œ ì‚¬ìš©)
        split: ë°ì´í„°ì…‹ split (ì˜ˆ: "train", "test")
        text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„ (Noneì´ë©´ ìë™ ê°ì§€)
        download_percentage: ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ì…‹ ë¹„ìœ¨ (1-100, Noneì´ë©´ num_samples ì‚¬ìš©)
        download_full: Trueë©´ ì „ì²´ ë‹¤ìš´ë¡œë“œ
        
    Returns:
        List[str]: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    try:
        from datasets import load_dataset
        
        # í¼ì„¼í‹°ì§€ ê¸°ë°˜ ë‹¤ìš´ë¡œë“œ
        if download_percentage is not None:
            actual_split = f"{split}[:{download_percentage}%]"
            num_samples_to_use = None
        elif not download_full:
            if "[" not in split:
                actual_split = f"{split}[:{num_samples}]"
            else:
                actual_split = split
            num_samples_to_use = num_samples
        else:
            actual_split = split
            num_samples_to_use = None
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        try:
            if download_full:
                # ì „ì²´ ë‹¤ìš´ë¡œë“œ
                dataset = load_dataset(dataset_name, split=split, trust_remote_code=True)
            else:
                # ì¼ë¶€ë§Œ ë‹¤ìš´ë¡œë“œ: streaming ëª¨ë“œ ì‚¬ìš©
                try:
                    dataset = load_dataset(
                        dataset_name, 
                        split=actual_split, 
                        trust_remote_code=True,
                        streaming=True  # í•„ìš”í•œ ë¶€ë¶„ë§Œ ìŠ¤íŠ¸ë¦¬ë°
                    )
                except Exception:
                    # streaming ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ëª¨ë“œ
                    dataset = load_dataset(
                        dataset_name, 
                        split=actual_split, 
                        trust_remote_code=True
                    )
        except Exception as e:
            try:
                if download_full:
                    dataset = load_dataset(dataset_name, split=split)
                else:
                    dataset = load_dataset(dataset_name, split=actual_split)
            except Exception:
                raise Exception(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {dataset_name}\nì—ëŸ¬: {e}")
        
        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ìë™ ê°ì§€
        if text_column is None:
            possible_columns = ['text', 'Text', 'review', 'content', 'sentence', 'document', 'abstract', 'body']
            text_column = None
            
            for col in possible_columns:
                if col in dataset.column_names:
                    text_column = col
                    break
            
            # ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë¬¸ìì—´ ì»¬ëŸ¼ ì°¾ê¸°
            if text_column is None:
                for col in dataset.column_names:
                    if col not in ['label', 'labels', 'id', 'idx']:
                        try:
                            sample = dataset[0][col]
                            if isinstance(sample, str):
                                text_column = col
                                break
                        except:
                            continue
        
        if text_column is None:
            raise ValueError(f"í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {dataset.column_names}")
        
        texts = []
        # Streaming ëª¨ë“œì¸ ê²½ìš° ì²˜ë¦¬
        is_streaming = hasattr(dataset, '__iter__') and not hasattr(dataset, '__len__')
        
        if is_streaming:
            # Streaming ëª¨ë“œ: í•„ìš”í•œ ê°œìˆ˜ë§Œ ìˆœíšŒ
            max_samples = num_samples_to_use if num_samples_to_use is not None else 100
            for i, item in enumerate(dataset):
                if i >= max_samples:
                    break
                try:
                    text = item[text_column]
                    if isinstance(text, str) and len(text.strip()) > 0:
                        texts.append(text)
                except Exception as e:
                    print(f"í…ìŠ¤íŠ¸ {i} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
        else:
            # ì¼ë°˜ ëª¨ë“œ: len() ì‚¬ìš© ê°€ëŠ¥
            max_samples = num_samples_to_use if num_samples_to_use is not None else len(dataset)
            for i in range(min(max_samples, len(dataset))):
                try:
                    text = dataset[i][text_column]
                    if isinstance(text, str) and len(text.strip()) > 0:
                        texts.append(text)
                except Exception as e:
                    print(f"í…ìŠ¤íŠ¸ {i} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
        
        return texts
    
    except ImportError:
        raise ImportError("datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install datasets ì‹¤í–‰í•˜ì„¸ìš”.")
    except Exception as e:
        raise Exception(f"Hugging Face í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨ ({dataset_name}): {e}")

def load_custom_dataset(folder_path: str, num_samples: int = 100):
    """
    ë¡œì»¬ í´ë”ì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        folder_path: ì´ë¯¸ì§€ê°€ ìˆëŠ” í´ë” ê²½ë¡œ
        num_samples: ìµœëŒ€ ë¡œë“œí•  ì´ë¯¸ì§€ ê°œìˆ˜
        
    Returns:
        List[PIL.Image]: ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
    """
    import os
    import glob
    
    if not os.path.exists(folder_path):
        raise FileNotFoundError(f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
    
    # ì§€ì›í•˜ëŠ” ì´ë¯¸ì§€ í™•ì¥ì
    extensions = ["*.jpg", "*.jpeg", "*.png", "*.bmp", "*.gif"]
    
    image_files = []
    for ext in extensions:
        image_files.extend(glob.glob(os.path.join(folder_path, ext)))
        image_files.extend(glob.glob(os.path.join(folder_path, ext.upper())))
    
    if not image_files:
        raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
    
    images = []
    for img_file in image_files[:num_samples]:
        try:
            img = Image.open(img_file)
            images.append(img)
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_file}, {e}")
            continue
    
    return images

