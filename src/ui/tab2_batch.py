"""ë°ì´í„°ì…‹ ë°°ì¹˜ ë¶„ì„ íƒ­"""
import streamlit as st
import pandas as pd
from datetime import datetime
from collections import Counter
from src.utils import get_grade, generate_dataset_report_pdf
from src.dataset_analyzer import (
    analyze_dataset_images, analyze_dataset_texts,
    load_cifar10, load_tid2013, load_custom_dataset,
    load_huggingface_dataset, load_huggingface_text_dataset
)
from src.dataset_finder import (
    search_huggingface_datasets, get_popular_datasets, get_predefined_datasets
)


def render_tab2(tab):
    st.header("ë°ì´í„°ì…‹ ë°°ì¹˜ ë¶„ì„")
    st.markdown("CIFAR-10, TID2013 ë“± ë°ì´í„°ì…‹ì„ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ í’ˆì§ˆì„ ë¶„ì„í•©ë‹ˆë‹¤.")
    # ë°ì´í„° íƒ€ì… ì„ íƒ
    data_type = st.radio(
        "ë°ì´í„° íƒ€ì…",
        ["ì´ë¯¸ì§€", "í…ìŠ¤íŠ¸"],
        horizontal=True
    )
    # ë°ì´í„°ì…‹ ì†ŒìŠ¤ ì„ íƒ (ë¯¸ë¦¬ íƒ‘ì¬ëœ ìƒ˜í”Œ vs Hugging Face ê²€ìƒ‰)
    dataset_source = st.radio(
        "ë°ì´í„°ì…‹ ì†ŒìŠ¤",
        ["ë¯¸ë¦¬ íƒ‘ì¬ëœ ìƒ˜í”Œ ë°ì´í„°ì…‹", "Hugging Faceì—ì„œ ê²€ìƒ‰"],
        horizontal=True,
        help="ë¯¸ë¦¬ íƒ‘ì¬ëœ ìƒ˜í”Œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ê±°ë‚˜ Hugging Faceì—ì„œ ê²€ìƒ‰í•˜ì—¬ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )
    if data_type == "ì´ë¯¸ì§€":
        if dataset_source == "ë¯¸ë¦¬ íƒ‘ì¬ëœ ìƒ˜í”Œ ë°ì´í„°ì…‹":
            dataset_option = st.selectbox(
                "ë¶„ì„í•  ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ì„ íƒ",
                [
                    "CIFAR-10 (torchvision)",
                    "Hugging Face: beans (ì½© ì§ˆë³‘ ë¶„ë¥˜)",
                    "Hugging Face: food101 (ìŒì‹ ì´ë¯¸ì§€)",
                    "Hugging Face: cifar10",
                    "Hugging Face: cifar100 (100 í´ë˜ìŠ¤)",
                    "Hugging Face: mnist (ì†ê¸€ì”¨ ìˆ«ì)",
                    "Hugging Face: fashion_mnist (íŒ¨ì…˜ ì•„ì´í…œ)",
                    "Hugging Face: imagenet-1k (ëŒ€ê·œëª¨ ì´ë¯¸ì§€ ë¶„ë¥˜)",
                    "Hugging Face: oxford-iiit-pet (ë°˜ë ¤ë™ë¬¼)",
                    "Hugging Face: oxford_flowers102 (ê½ƒ ì´ë¯¸ì§€)",
                    "Hugging Face: stanford-cars (ìë™ì°¨)",
                    "Hugging Face: celeba (ì¸ë¬¼ ì–¼êµ´)",
                    "Hugging Face: coco (ê°ì²´ íƒì§€)",
                    "TID2013 (ë¡œì»¬)",
                    "ì»¤ìŠ¤í…€ í´ë”"
                ]
            )
        else:  # Hugging Face ê²€ìƒ‰
            dataset_option = "Hugging Face: ê²€ìƒ‰"
    else:  # í…ìŠ¤íŠ¸
        if dataset_source == "ë¯¸ë¦¬ íƒ‘ì¬ëœ ìƒ˜í”Œ ë°ì´í„°ì…‹":
            dataset_option = st.selectbox(
                "ë¶„ì„í•  í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„ íƒ",
                [
                    "Hugging Face: imdb (ì˜í™” ë¦¬ë·°)",
                    "Hugging Face: yelp_review_full (ë¦¬ë·°)",
                    "Hugging Face: ag_news (ë‰´ìŠ¤ ë¶„ë¥˜)",
                    "Hugging Face: amazon_polarity (ì•„ë§ˆì¡´ ë¦¬ë·°)",
                    "Hugging Face: sst2 (ê°ì • ë¶„ì„)",
                    "Hugging Face: squad (ì§ˆë¬¸ ë‹µë³€)",
                    "Hugging Face: nyu-mll/glue (GLUE ë²¤ì¹˜ë§ˆí¬)",
                    "Hugging Face: super_glue (SuperGLUE)",
                    "Hugging Face: wikitext (ìœ„í‚¤í…ìŠ¤íŠ¸)",
                    "Hugging Face: emotion (ê°ì • ë¶„ë¥˜)",
                    "Hugging Face: rotten_tomatoes (ì˜í™” ë¦¬ë·°)",
                    "Hugging Face: tweet_eval (íŠ¸ìœ„í„° ê°ì •)",
                    "Hugging Face: multi_nli (ìì—°ì–´ ì¶”ë¡ )",
                    "Hugging Face: 20newsgroups (ë‰´ìŠ¤ê·¸ë£¹)"
                ]
            )
        else:  # Hugging Face ê²€ìƒ‰
            dataset_option = "Hugging Face: ê²€ìƒ‰"
    # ë¯¸ë¦¬ ì •ì˜ëœ Hugging Face ë°ì´í„°ì…‹ì¸ ê²½ìš° split ì„ íƒ (ë‹¤ìš´ë¡œë“œ ì „)
    if dataset_option.startswith("Hugging Face:") and dataset_option != "Hugging Face: ê²€ìƒ‰":
        hf_dataset_name = None
        hf_text_dataset_name = None
        if data_type == "ì´ë¯¸ì§€":
            if "beans" in dataset_option:
                hf_dataset_name = "beans"
            elif "food101" in dataset_option:
                hf_dataset_name = "food101"
            elif "cifar10" in dataset_option and "cifar100" not in dataset_option:
                hf_dataset_name = "cifar10"
            elif "cifar100" in dataset_option:
                hf_dataset_name = "cifar100"
            elif "mnist" in dataset_option and "fashion" not in dataset_option:
                hf_dataset_name = "mnist"
            elif "fashion_mnist" in dataset_option or "fashion" in dataset_option:
                hf_dataset_name = "fashion_mnist"
            elif "imagenet" in dataset_option:
                hf_dataset_name = "imagenet-1k"
            elif "oxford-iiit-pet" in dataset_option or "pet" in dataset_option.lower():
                hf_dataset_name = "oxford-iiit-pet"
            elif "oxford_flowers" in dataset_option or "flowers" in dataset_option.lower():
                hf_dataset_name = "oxford_flowers102"
            elif "stanford-cars" in dataset_option or "cars" in dataset_option.lower():
                hf_dataset_name = "stanford-cars"
            elif "celeba" in dataset_option:
                hf_dataset_name = "celeba"
            elif "coco" in dataset_option:
                hf_dataset_name = "coco"
        else:  # í…ìŠ¤íŠ¸
            if "imdb" in dataset_option:
                hf_text_dataset_name = "imdb"
            elif "yelp" in dataset_option:
                hf_text_dataset_name = "yelp_review_full"
            elif "ag_news" in dataset_option:
                hf_text_dataset_name = "ag_news"
            elif "amazon_polarity" in dataset_option or ("amazon" in dataset_option and "polarity" in dataset_option):
                hf_text_dataset_name = "amazon_polarity"
            elif "sst2" in dataset_option:
                hf_text_dataset_name = "sst2"
            elif "squad" in dataset_option and "super" not in dataset_option:
                hf_text_dataset_name = "squad"
            elif "glue" in dataset_option and "super" not in dataset_option:
                hf_text_dataset_name = "nyu-mll/glue"
            elif "super_glue" in dataset_option or "superglue" in dataset_option.lower():
                hf_text_dataset_name = "super_glue"
            elif "wikitext" in dataset_option:
                hf_text_dataset_name = "wikitext"
            elif "emotion" in dataset_option:
                hf_text_dataset_name = "emotion"
            elif "rotten_tomatoes" in dataset_option or "rotten" in dataset_option.lower():
                hf_text_dataset_name = "rotten_tomatoes"
            elif "tweet_eval" in dataset_option or "tweet" in dataset_option.lower():
                hf_text_dataset_name = "tweet_eval"
            elif "multi_nli" in dataset_option or "nli" in dataset_option.lower():
                hf_text_dataset_name = "multi_nli"
            elif "20newsgroups" in dataset_option or "newsgroups" in dataset_option.lower():
                hf_text_dataset_name = "20newsgroups"
        # ========== Split ì„ íƒ ê¸°ëŠ¥ (í˜„ì¬ ë¹„í™œì„±í™” - ì£¼ì„ ì²˜ë¦¬) ==========
        # Splitì€ ìë™ìœ¼ë¡œ ì„ íƒë©ë‹ˆë‹¤ (ê¸°ë³¸ê°’: train, ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ split ì„ íƒ)
        # ì•„ë˜ ì½”ë“œë¥¼ ì£¼ì„ í•´ì œí•˜ë©´ split ì„ íƒ UIê°€ í™œì„±í™”ë©ë‹ˆë‹¤
        # 
        # if hf_dataset_name or hf_text_dataset_name:
        #     dataset_name = hf_dataset_name or hf_text_dataset_name
        #     from src.dataset_analyzer import get_available_splits
        #     try:
        #         available_splits = get_available_splits(dataset_name)
        #         default_split = "train" if "train" in available_splits else available_splits[0] if available_splits else "train"
        #         split_index = available_splits.index(default_split) if default_split in available_splits else 0
        #         
        #         split_key = "img_split_predefined" if data_type == "ì´ë¯¸ì§€" else "text_split_predefined"
        #         selected_split = st.selectbox(
        #             "Split ì„ íƒ (ë‹¤ìš´ë¡œë“œ ì „ì— ì„ íƒ)",
        #             available_splits,
        #             index=split_index,
        #             key=split_key,
        #             help="train: í•™ìŠµìš© ë°ì´í„°, test: í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°, validation/val: ê²€ì¦ìš© ë°ì´í„°"
        #         )
        #         if data_type == "ì´ë¯¸ì§€":
        #             st.session_state['selected_img_split_predefined'] = selected_split
        #         else:
        #             st.session_state['selected_text_split_predefined'] = selected_split
        #     except Exception:
        #         available_splits = ["train", "test", "validation", "val"]
        #         split_key = "img_split_predefined" if data_type == "ì´ë¯¸ì§€" else "text_split_predefined"
        #         selected_split = st.selectbox(
        #             "Split ì„ íƒ (ë‹¤ìš´ë¡œë“œ ì „ì— ì„ íƒ)",
        #             available_splits,
        #             index=0,
        #             key=split_key,
        #             help="train: í•™ìŠµìš© ë°ì´í„°, test: í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°, validation/val: ê²€ì¦ìš© ë°ì´í„°"
        #         )
        #         if data_type == "ì´ë¯¸ì§€":
        #             st.session_state['selected_img_split_predefined'] = selected_split
        #         else:
        #             st.session_state['selected_text_split_predefined'] = selected_split
        st.divider()
    # Hugging Face ê²€ìƒ‰ì¸ ê²½ìš° ë¨¼ì € ê²€ìƒ‰ UI í‘œì‹œ
    if dataset_option == "Hugging Face: ê²€ìƒ‰":
        st.subheader("Hugging Face ë°ì´í„°ì…‹ ê²€ìƒ‰")
        if data_type == "ì´ë¯¸ì§€":
            # ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ì¸ê¸° ëª©ë¡ ë° ê²€ìƒ‰
            col_info, col_search = st.columns([2, 3])
            with col_info:
                st.markdown("### ì¸ê¸° ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ëª©ë¡")
                try:
                    popular_list = get_predefined_datasets("image-classification")
                    if popular_list:
                        st.dataframe({
                            "ë°ì´í„°ì…‹ ID": [d["id"] for d in popular_list[:20]],
                            "ì‘ì„±ì": [d.get("author", "-") for d in popular_list[:20]],
                            "ë‹¤ìš´ë¡œë“œ ìˆ˜": [f"{d.get('downloads', 0):,}" for d in popular_list[:20]],
                        },
                        use_container_width=True,
                        height=300,
                        key="img_popular_list"
                        )
                except Exception as e:
                    st.info(f"ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            with col_search:
                search_query = st.text_input(
                    "ê²€ìƒ‰ì–´ ì…ë ¥",
                    placeholder="ì˜ˆ: cats, dogs, classification",
                    help="ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ê²€ìƒ‰",
                    key="img_search_main"
                )
                if st.button("ê²€ìƒ‰", key="img_search_btn_main", use_container_width=True):
                    st.session_state['img_search_active'] = True
                if st.session_state.get('img_search_active', False) or search_query:
                    try:
                        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
                            if search_query:
                                results = search_huggingface_datasets(
                                    query=search_query,
                                    task="image-classification",
                                    max_results=30
                                )
                            else:
                                results = get_popular_datasets(
                                    task="image-classification",
                                    max_results=30
                                )
                        if results:
                            st.success(f"{len(results)}ê°œ ë°ì´í„°ì…‹ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                            dataset_df = st.dataframe({
                                "ë°ì´í„°ì…‹ ID": [d["id"] for d in results],
                                "ì‘ì„±ì": [d.get("author", "-") for d in results],
                                "ë‹¤ìš´ë¡œë“œ ìˆ˜": [f"{d.get('downloads', 0):,}" for d in results],
                            },
                            use_container_width=True,
                            height=300,
                            key="img_search_results"
                            )
                            # ì„ íƒëœ ë°ì´í„°ì…‹ í‘œì‹œ
                            selected_id = st.text_input(
                                "ë¶„ì„í•  ë°ì´í„°ì…‹ ID ì…ë ¥",
                                value=st.session_state.get('selected_img_dataset', ''),
                                help="ìœ„ ëª©ë¡ì—ì„œ ë°ì´í„°ì…‹ IDë¥¼ ë³µì‚¬í•˜ì—¬ ì…ë ¥í•˜ì„¸ìš”",
                                key="img_dataset_id_input"
                            )
                            if selected_id:
                                st.session_state['selected_img_dataset'] = selected_id
                                # ========== Split ì„ íƒ ê¸°ëŠ¥ (í˜„ì¬ ë¹„í™œì„±í™” - ì£¼ì„ ì²˜ë¦¬) ==========
                                # Splitì€ ìë™ìœ¼ë¡œ ì„ íƒë©ë‹ˆë‹¤ (ê¸°ë³¸ê°’: train, ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ split ì„ íƒ)
                                # ì•„ë˜ ì½”ë“œë¥¼ ì£¼ì„ í•´ì œí•˜ë©´ split ì„ íƒ UIê°€ í™œì„±í™”ë©ë‹ˆë‹¤
                                # 
                                # from src.dataset_analyzer import get_available_splits
                                # try:
                                #     available_splits = get_available_splits(selected_id)
                                #     default_split = "train" if "train" in available_splits else available_splits[0] if available_splits else "train"
                                #     split_index = available_splits.index(default_split) if default_split in available_splits else 0
                                #     
                                #     selected_split = st.selectbox(
                                #         "Split ì„ íƒ (ë‹¤ìš´ë¡œë“œ ì „ì— ì„ íƒ)",
                                #         available_splits,
                                #         index=split_index,
                                #         key="img_split_preview",
                                #         help="train: í•™ìŠµìš© ë°ì´í„°, test: í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°, validation/val: ê²€ì¦ìš© ë°ì´í„°"
                                #     )
                                #     st.session_state['selected_img_split'] = selected_split
                                # except Exception as e:
                                #     st.warning(f"Split ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(train)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {e}")
                                #     st.session_state['selected_img_split'] = "train"
                        else:
                            if search_query:
                                st.warning(f"'{search_query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
                            else:
                                st.info("ì¸ê¸° ë°ì´í„°ì…‹ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    except Exception as e:
                        st.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        else:  # í…ìŠ¤íŠ¸
            # í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¸ê¸° ëª©ë¡ ë° ê²€ìƒ‰
            col_info, col_search = st.columns([2, 3])
            with col_info:
                st.markdown("### ì¸ê¸° í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ ëª©ë¡")
                try:
                    popular_list = get_predefined_datasets("text-classification")
                    if popular_list:
                        st.dataframe({
                            "ë°ì´í„°ì…‹ ID": [d["id"] for d in popular_list[:20]],
                            "ì‘ì„±ì": [d.get("author", "-") for d in popular_list[:20]],
                            "ë‹¤ìš´ë¡œë“œ ìˆ˜": [f"{d.get('downloads', 0):,}" for d in popular_list[:20]],
                        },
                        use_container_width=True,
                        height=300,
                        key="text_popular_list"
                        )
                except Exception as e:
                    st.info(f"ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            with col_search:
                search_query = st.text_input(
                    "ê²€ìƒ‰ì–´ ì…ë ¥",
                    placeholder="ì˜ˆ: sentiment, review, classification",
                    help="í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê²€ìƒ‰",
                    key="text_search_main"
                )
                if st.button("ê²€ìƒ‰", key="text_search_btn_main", use_container_width=True):
                    st.session_state['text_search_active'] = True
                if st.session_state.get('text_search_active', False) or search_query:
                    try:
                        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
                            if search_query:
                                results = search_huggingface_datasets(
                                    query=search_query,
                                    task="text-classification",
                                    max_results=30
                                )
                            else:
                                results = get_popular_datasets(
                                    task="text-classification",
                                    max_results=30
                                )
                        if results:
                            st.success(f"{len(results)}ê°œ ë°ì´í„°ì…‹ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                            dataset_df = st.dataframe({
                                "ë°ì´í„°ì…‹ ID": [d["id"] for d in results],
                                "ì‘ì„±ì": [d.get("author", "-") for d in results],
                                "ë‹¤ìš´ë¡œë“œ ìˆ˜": [f"{d.get('downloads', 0):,}" for d in results],
                            },
                            use_container_width=True,
                            height=300,
                            key="text_search_results"
                            )
                            # ì„ íƒëœ ë°ì´í„°ì…‹ í‘œì‹œ
                            selected_id = st.text_input(
                                "ë¶„ì„í•  ë°ì´í„°ì…‹ ID ì…ë ¥",
                                value=st.session_state.get('selected_text_dataset', ''),
                                help="ìœ„ ëª©ë¡ì—ì„œ ë°ì´í„°ì…‹ IDë¥¼ ë³µì‚¬í•˜ì—¬ ì…ë ¥í•˜ì„¸ìš”",
                                key="text_dataset_id_input"
                            )
                            if selected_id:
                                st.session_state['selected_text_dataset'] = selected_id
                                # ========== Split ì„ íƒ ê¸°ëŠ¥ (í˜„ì¬ ë¹„í™œì„±í™” - ì£¼ì„ ì²˜ë¦¬) ==========
                                # Splitì€ ìë™ìœ¼ë¡œ ì„ íƒë©ë‹ˆë‹¤ (ê¸°ë³¸ê°’: train, ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ split ì„ íƒ)
                                # ì•„ë˜ ì½”ë“œë¥¼ ì£¼ì„ í•´ì œí•˜ë©´ split ì„ íƒ UIê°€ í™œì„±í™”ë©ë‹ˆë‹¤
                                # 
                                # from src.dataset_analyzer import get_available_splits
                                # try:
                                #     available_splits = get_available_splits(selected_id)
                                #     default_split = "train" if "train" in available_splits else available_splits[0] if available_splits else "train"
                                #     split_index = available_splits.index(default_split) if default_split in available_splits else 0
                                #     
                                #     selected_split = st.selectbox(
                                #         "Split ì„ íƒ (ë‹¤ìš´ë¡œë“œ ì „ì— ì„ íƒ)",
                                #         available_splits,
                                #         index=split_index,
                                #         key="text_split_preview",
                                #         help="train: í•™ìŠµìš© ë°ì´í„°, test: í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°, validation/val: ê²€ì¦ìš© ë°ì´í„°"
                                #     )
                                #     st.session_state['selected_text_split'] = selected_split
                                # except Exception as e:
                                #     st.warning(f"Split ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(train)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {e}")
                                #     st.session_state['selected_text_split'] = "train"
                        else:
                            if search_query:
                                st.warning(f"'{search_query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
                            else:
                                st.info("ì¸ê¸° ë°ì´í„°ì…‹ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    except Exception as e:
                        st.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        st.divider()
    # ë‹¤ìš´ë¡œë“œ ì„¤ì •
    download_mode = st.radio(
        "ë‹¤ìš´ë¡œë“œ ë°©ì‹",
        ["ìƒ˜í”Œ ê°œìˆ˜ ì§€ì •", "ì „ì²´ ë°ì´í„°ì…‹ í¼ì„¼í‹°ì§€", "ì „ì²´ ë‹¤ìš´ë¡œë“œ"],
        help="ì¼ë¶€ë§Œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥"
    )
    if download_mode == "ìƒ˜í”Œ ê°œìˆ˜ ì§€ì •":
        num_samples = st.slider("ë¶„ì„í•  ìƒ˜í”Œ ê°œìˆ˜", min_value=10, max_value=500, value=100, step=10)
        download_percentage = None
        download_full = False
    elif download_mode == "ì „ì²´ ë‹¤ìš´ë¡œë“œ":
        st.info("ì „ì²´ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. (ì‹œê°„ì´ ë§ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        num_samples = None
        download_percentage = None
        download_full = True
    else:  # í¼ì„¼í‹°ì§€ ëª¨ë“œ
        download_percentage = st.slider(
            "ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ì…‹ ë¹„ìœ¨ (%)",
            min_value=1,
            max_value=100,
            value=10,
            step=1,
            help="ì˜ˆ: 10% = ì „ì²´ ë°ì´í„°ì…‹ì˜ 10%ë§Œ ë‹¤ìš´ë¡œë“œ, 100% = ì „ì²´ ë‹¤ìš´ë¡œë“œ"
        )
        if download_percentage == 100:
            st.info("100% ì„ íƒ = ì „ì²´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
        num_samples = None  # í¼ì„¼í‹°ì§€ ì‚¬ìš© ì‹œ ìƒ˜í”Œ ê°œìˆ˜ëŠ” ìë™ ê³„ì‚°
        download_full = False
    if st.button("ë°ì´í„°ì…‹ ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True):
        try:
            with st.spinner(f"{dataset_option} ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                images = []
                texts = []
                if dataset_option == "CIFAR-10 (torchvision)":
                    dataset_option = "CIFAR-10"  # ì²˜ë¦¬ ë¡œì§ í˜¸í™˜ì„±
                if dataset_option == "CIFAR-10":
                    if download_full:
                        # ì „ì²´ ë‹¤ìš´ë¡œë“œ: CIFAR-10ì€ ì´ 50,000ì¥
                        st.info("CIFAR-10 ì „ì²´ ë°ì´í„°ì…‹ (50,000ì¥) ë‹¤ìš´ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                        images = load_cifar10(50000)
                    elif download_percentage:
                        # í¼ì„¼í‹°ì§€ ê¸°ë°˜: CIFAR-10ì€ ì´ 50,000ì¥ì´ë¯€ë¡œ ê³„ì‚°
                        total_cifar = 50000
                        calculated_samples = int(total_cifar * download_percentage / 100)
                        st.info(f"CIFAR-10 ë°ì´í„°ì…‹ì˜ {download_percentage}% ({calculated_samples}ì¥) ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
                        images = load_cifar10(calculated_samples)
                    else:
                        st.info("CIFAR-10 ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. (ì²« ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)")
                        images = load_cifar10(num_samples)
                    st.success(f"CIFAR-10 ë°ì´í„°ì…‹ {len(images)}ê°œ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ!")
                elif dataset_option == "Hugging Face: ê²€ìƒ‰":
                    # ê²€ìƒ‰ìœ¼ë¡œ ì„ íƒí•œ ë°ì´í„°ì…‹ ì‚¬ìš©
                    if data_type == "ì´ë¯¸ì§€":
                        hf_dataset_name = st.session_state.get('selected_img_dataset', '')
                        if not hf_dataset_name:
                            st.error("ë°ì´í„°ì…‹ IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                            st.stop()
                        # Splitì€ ìë™ìœ¼ë¡œ ì„ íƒë©ë‹ˆë‹¤ (ê¸°ë³¸ê°’: train, ì—†ìœ¼ë©´ ì‚¬ìš© ê°€ëŠ¥í•œ split ìë™ ì„ íƒ)
                        split_name = "train"  # ê¸°ë³¸ê°’, dataset_analyzerì—ì„œ ìë™ ì¡°ì •
                    else:  # í…ìŠ¤íŠ¸
                        hf_dataset_name = st.session_state.get('selected_text_dataset', '')
                        if not hf_dataset_name:
                            st.error("ë°ì´í„°ì…‹ IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                            st.stop()
                        # Splitì€ ìë™ìœ¼ë¡œ ì„ íƒë©ë‹ˆë‹¤ (ê¸°ë³¸ê°’: train, ì—†ìœ¼ë©´ ì‚¬ìš© ê°€ëŠ¥í•œ split ìë™ ì„ íƒ)
                        split_name = "train"  # ê¸°ë³¸ê°’, dataset_analyzerì—ì„œ ìë™ ì¡°ì •
                    # ê²€ìƒ‰ìœ¼ë¡œ ì„ íƒí•œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
                    if download_full:
                        if data_type == "ì´ë¯¸ì§€":
                            st.info(f"{hf_dataset_name} ì „ì²´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ë§ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                            images = load_huggingface_dataset(
                                hf_dataset_name,
                                num_samples=None,
                                split=split_name,
                                download_full=True
                            )
                            st.success(f"Hugging Face '{hf_dataset_name}' ì´ë¯¸ì§€ ë°ì´í„°ì…‹ {len(images)}ê°œ ë¡œë“œ ì™„ë£Œ!")
                        else:
                            st.info(f"{hf_dataset_name} ì „ì²´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ë§ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                            texts = load_huggingface_text_dataset(
                                hf_dataset_name,
                                num_samples=None,
                                split=split_name,
                                download_full=True
                            )
                            st.success(f"Hugging Face '{hf_dataset_name}' í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ {len(texts)}ê°œ ë¡œë“œ ì™„ë£Œ!")
                    elif download_percentage:
                        if data_type == "ì´ë¯¸ì§€":
                            st.info(f"{hf_dataset_name} ë°ì´í„°ì…‹ì˜ {download_percentage}% ë‹¤ìš´ë¡œë“œ ì¤‘...")
                            images = load_huggingface_dataset(
                                hf_dataset_name,
                                num_samples=None,
                                split=split_name,
                                download_percentage=download_percentage
                            )
                            st.success(f"Hugging Face '{hf_dataset_name}' ì´ë¯¸ì§€ ë°ì´í„°ì…‹ {len(images)}ê°œ ë¡œë“œ ì™„ë£Œ!")
                        else:
                            st.info(f"{hf_dataset_name} ë°ì´í„°ì…‹ì˜ {download_percentage}% ë‹¤ìš´ë¡œë“œ ì¤‘...")
                            texts = load_huggingface_text_dataset(
                                hf_dataset_name,
                                num_samples=None,
                                split=split_name,
                                download_percentage=download_percentage
                            )
                            st.success(f"Hugging Face '{hf_dataset_name}' í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ {len(texts)}ê°œ ë¡œë“œ ì™„ë£Œ!")
                    else:
                        if data_type == "ì´ë¯¸ì§€":
                            st.info(f"{hf_dataset_name} ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                            images = load_huggingface_dataset(
                                hf_dataset_name,
                                num_samples=num_samples,
                                split=split_name
                            )
                            st.success(f"Hugging Face '{hf_dataset_name}' ì´ë¯¸ì§€ ë°ì´í„°ì…‹ {len(images)}ê°œ ë¡œë“œ ì™„ë£Œ!")
                        else:
                            st.info(f"{hf_dataset_name} ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                            texts = load_huggingface_text_dataset(
                                hf_dataset_name,
                                num_samples=num_samples,
                                split=split_name
                            )
                            st.success(f"Hugging Face '{hf_dataset_name}' í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ {len(texts)}ê°œ ë¡œë“œ ì™„ë£Œ!")
                elif dataset_option.startswith("Hugging Face:"):
                    hf_dataset_name = None
                    hf_text_dataset_name = None
                    split_name = "train"
                    if data_type == "ì´ë¯¸ì§€":
                        if "beans" in dataset_option:
                            hf_dataset_name = "beans"
                            if download_percentage:
                                st.info(f"Beans ë°ì´í„°ì…‹ì˜ {download_percentage}% ë‹¤ìš´ë¡œë“œ ì¤‘... (ì½© ì§ˆë³‘ ë¶„ë¥˜ ì´ë¯¸ì§€)")
                            else:
                                st.info("Beans ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì½© ì§ˆë³‘ ë¶„ë¥˜ ì´ë¯¸ì§€)")
                        elif "food101" in dataset_option:
                            hf_dataset_name = "food101"
                            if download_percentage:
                                st.info(f"Food-101 ë°ì´í„°ì…‹ì˜ {download_percentage}% ë‹¤ìš´ë¡œë“œ ì¤‘... (ìŒì‹ ì´ë¯¸ì§€)")
                            else:
                                st.info("Food-101 ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘... (ìŒì‹ ì´ë¯¸ì§€)")
                        elif "cifar10" in dataset_option:
                            hf_dataset_name = "cifar10"
                            if download_percentage:
                                st.info(f"CIFAR-10 (Hugging Face) ë°ì´í„°ì…‹ì˜ {download_percentage}% ë‹¤ìš´ë¡œë“œ ì¤‘...")
                            else:
                                st.info("CIFAR-10 (Hugging Face) ë‹¤ìš´ë¡œë“œ ì¤‘...")
                        # ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬
                        if hf_dataset_name:
                            # Splitì€ ìë™ìœ¼ë¡œ ì„ íƒë©ë‹ˆë‹¤ (ê¸°ë³¸ê°’: train)
                            split_name = "train"  # ê¸°ë³¸ê°’, dataset_analyzerì—ì„œ ìë™ ì¡°ì •
                            if download_full:
                                images = load_huggingface_dataset(
                                    hf_dataset_name,
                                    num_samples=None,
                                    split=split_name,
                                    download_full=True
                                )
                                st.info(f"{hf_dataset_name} ì „ì²´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ë§ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                            elif download_percentage:
                                images = load_huggingface_dataset(
                                    hf_dataset_name,
                                    num_samples=None,
                                    split=split_name,
                                    download_percentage=download_percentage
                                )
                            else:
                                images = load_huggingface_dataset(
                                    hf_dataset_name,
                                    num_samples=num_samples,
                                    split=split_name
                                )
                            st.success(f"Hugging Face '{hf_dataset_name}' ë°ì´í„°ì…‹ {len(images)}ê°œ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ!")
                    elif data_type == "í…ìŠ¤íŠ¸":
                        if "imdb" in dataset_option:
                            hf_text_dataset_name = "imdb"
                            if download_percentage:
                                st.info(f"IMDB ë°ì´í„°ì…‹ì˜ {download_percentage}% ë‹¤ìš´ë¡œë“œ ì¤‘... (ì˜í™” ë¦¬ë·° í…ìŠ¤íŠ¸)")
                            elif download_full:
                                st.info("IMDB ì „ì²´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì˜í™” ë¦¬ë·° í…ìŠ¤íŠ¸)")
                            else:
                                st.info("IMDB ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì˜í™” ë¦¬ë·° í…ìŠ¤íŠ¸)")
                        elif "yelp" in dataset_option:
                            hf_text_dataset_name = "yelp_review_full"
                            if download_percentage:
                                st.info(f"Yelp Review ë°ì´í„°ì…‹ì˜ {download_percentage}% ë‹¤ìš´ë¡œë“œ ì¤‘... (ë¦¬ë·° í…ìŠ¤íŠ¸)")
                            elif download_full:
                                st.info("Yelp Review ì „ì²´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘... (ë¦¬ë·° í…ìŠ¤íŠ¸)")
                            else:
                                st.info("Yelp Review ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘... (ë¦¬ë·° í…ìŠ¤íŠ¸)")
                        elif "ag_news" in dataset_option:
                            hf_text_dataset_name = "ag_news"
                            if download_percentage:
                                st.info(f"AG News ë°ì´í„°ì…‹ì˜ {download_percentage}% ë‹¤ìš´ë¡œë“œ ì¤‘... (ë‰´ìŠ¤ ë¶„ë¥˜ í…ìŠ¤íŠ¸)")
                            elif download_full:
                                st.info("AG News ì „ì²´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘... (ë‰´ìŠ¤ ë¶„ë¥˜ í…ìŠ¤íŠ¸)")
                            else:
                                st.info("AG News ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘... (ë‰´ìŠ¤ ë¶„ë¥˜ í…ìŠ¤íŠ¸)")
                        # í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬
                        if hf_text_dataset_name:
                            # Splitì€ ìë™ìœ¼ë¡œ ì„ íƒë©ë‹ˆë‹¤ (ê¸°ë³¸ê°’: train)
                            split_name = "train"  # ê¸°ë³¸ê°’, dataset_analyzerì—ì„œ ìë™ ì¡°ì •
                            if download_full:
                                texts = load_huggingface_text_dataset(
                                    hf_text_dataset_name,
                                    num_samples=None,
                                    split=split_name,
                                    download_full=True
                                )
                            elif download_percentage:
                                texts = load_huggingface_text_dataset(
                                    hf_text_dataset_name,
                                    num_samples=None,
                                    split=split_name,
                                    download_percentage=download_percentage
                                )
                            else:
                                texts = load_huggingface_text_dataset(
                                    hf_text_dataset_name,
                                    num_samples=num_samples,
                                    split=split_name
                                )
                            st.success(f"Hugging Face '{hf_text_dataset_name}' í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ {len(texts)}ê°œ ë¡œë“œ ì™„ë£Œ!")
                elif dataset_option == "TID2013 (ë¡œì»¬)":
                    # ì»¤ìŠ¤í…€ ê²½ë¡œ ì…ë ¥ ì˜µì…˜
                    use_custom_path = st.checkbox("ì»¤ìŠ¤í…€ ê²½ë¡œ ì‚¬ìš©", key="tid_custom_path")
                    custom_path = None
                    if use_custom_path:
                        custom_path = st.text_input(
                            "TID2013 ë°ì´í„°ì…‹ ê²½ë¡œ ì…ë ¥",
                            value="./data/TID2013",
                            help="TID2013 í´ë” ë˜ëŠ” reference_images í´ë”ì˜ ìƒìœ„ ê²½ë¡œ"
                        )
                    # TID2013ì€ ë¡œì»¬ íŒŒì¼ì´ë¯€ë¡œ í¼ì„¼í‹°ì§€ëŠ” ìƒ˜í”Œ ê°œìˆ˜ë¡œ ë³€í™˜
                    if download_percentage:
                        # TID2013 reference ì´ë¯¸ì§€ëŠ” ë³´í†µ 25ê°œ ì •ë„
                        total_tid = 25
                        calculated_samples = max(1, int(total_tid * download_percentage / 100))
                        st.info(f"TID2013 ë°ì´í„°ì…‹ì˜ {download_percentage}% ({calculated_samples}ì¥) ë¡œë“œí•©ë‹ˆë‹¤.")
                        images = load_tid2013(calculated_samples, custom_path=custom_path if use_custom_path else None)
                    else:
                        images = load_tid2013(num_samples, custom_path=custom_path if use_custom_path else None)
                    st.success(f"TID2013 ë°ì´í„°ì…‹ {len(images)}ê°œ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ!")
                elif dataset_option == "ì»¤ìŠ¤í…€ í´ë”":
                    folder_path = st.text_input("ì´ë¯¸ì§€ í´ë” ê²½ë¡œ ì…ë ¥", value="./data/images")
                    if folder_path:
                        # ì»¤ìŠ¤í…€ í´ë”ëŠ” í¼ì„¼í‹°ì§€ ê³„ì‚°ì´ ì–´ë ¤ìš°ë¯€ë¡œ ìƒ˜í”Œ ê°œìˆ˜ ì‚¬ìš©
                        if download_percentage:
                            st.warning("ì»¤ìŠ¤í…€ í´ë”ëŠ” í¼ì„¼í‹°ì§€ ëŒ€ì‹  ìƒ˜í”Œ ê°œìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                            images = load_custom_dataset(folder_path, num_samples if num_samples else 100)
                        else:
                            images = load_custom_dataset(folder_path, num_samples)
                        st.success(f"ì»¤ìŠ¤í…€ í´ë”ì—ì„œ {len(images)}ê°œ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ!")
                    else:
                        st.warning("í´ë” ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                        images = []
                # ì´ë¯¸ì§€ ë¶„ì„
                if images:
                    # ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    results = analyze_dataset_images(images, max_samples=num_samples)
                    progress_bar.progress(100)
                    status_text.text("ë¶„ì„ ì™„ë£Œ!")
                    # ê²°ê³¼ í‘œì‹œ
                    col1, col2 = st.columns(2)
                    with col1:
                        st.subheader("ì „ì²´ í†µê³„")
                        # ê¸´ ë¦¬ìŠ¤íŠ¸ëŠ” ì œì™¸í•˜ê³  ìš”ì•½ ì •ë³´ë§Œ í‘œì‹œ
                        filtered_results = {}
                        exclude_keys = ["ê°œë³„ ì ìˆ˜", "í•´ìƒë„ ëª©ë¡"]  # ë„ˆë¬´ ê¸´ ë¦¬ìŠ¤íŠ¸ ì œì™¸
                        for key, value in results.items():
                            if key not in exclude_keys:
                                # í•´ìƒë„ ë¶„í¬ ê°™ì€ ë”•ì…”ë„ˆë¦¬ëŠ” í‘œì‹œ
                                if isinstance(value, dict) and key == "í•´ìƒë„ ë¶„í¬":
                                    filtered_results[key] = str(value)
                                # ë‹¤ë¥¸ ê¸´ ë¦¬ìŠ¤íŠ¸ëŠ” ì œì™¸
                                elif isinstance(value, list) and len(value) > 20:
                                    filtered_results[key] = f"{len(value)}ê°œ í•­ëª©"
                                else:
                                    filtered_results[key] = value
                        st.dataframe({
                            "ì§€í‘œ": list(filtered_results.keys()),
                            "ê°’": [str(v) for v in filtered_results.values()]
                        },
                        use_container_width=True
                        )
                    with col2:
                        avg_score = results.get("í‰ê·  ì¢…í•© ì ìˆ˜", 0.0)
                        grade = get_grade(avg_score)
                        st.metric("í‰ê·  í’ˆì§ˆ ì ìˆ˜", f"{avg_score:.3f}")
                        st.metric("í’ˆì§ˆ ë“±ê¸‰", grade)
                        if grade == "A":
                            st.success("ìš°ìˆ˜í•œ í’ˆì§ˆì˜ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤!")
                        elif grade == "B":
                            st.info("ì–‘í˜¸í•œ í’ˆì§ˆì˜ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.")
                        elif grade == "C":
                            st.warning("í’ˆì§ˆ ê°œì„ ì´ ê¶Œì¥ë©ë‹ˆë‹¤.")
                        else:
                            st.error("í’ˆì§ˆ ê°œì„ ì´ ì‹œê¸‰í•©ë‹ˆë‹¤.")
                    # ìƒì„¸ ì§€í‘œ ì‹œê°í™”
                    st.subheader("í’ˆì§ˆ ì§€í‘œ ìƒì„¸")
                    metrics_data = {
                        "í‰ê·  í•´ìƒë„": results["í‰ê·  í•´ìƒë„"],
                        "í‰ê·  ìœ íš¨ì„±": results["í‰ê·  ìœ íš¨ì„±"],
                        "í‰ê·  ë‹¤ì–‘ì„±": results["í‰ê·  ë‹¤ì–‘ì„±"],
                    }
                    st.bar_chart(metrics_data)
                    # í•´ìƒë„ ë¶„í¬ ì •ë³´ í‘œì‹œ
                    if "í•´ìƒë„ ë¶„í¬" in results:
                        st.subheader("ì„ íƒëœ ì´ë¯¸ì§€ í•´ìƒë„ ì •ë³´")
                        resolution_info = results["í•´ìƒë„ ë¶„í¬"]
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("ìµœì†Œ í•´ìƒë„", resolution_info["ìµœì†Œ"])
                        with col2:
                            st.metric("ìµœëŒ€ í•´ìƒë„", resolution_info["ìµœëŒ€"])
                        with col3:
                            st.metric("í‰ê·  í•´ìƒë„", resolution_info["í‰ê· "])
                        with col4:
                            st.metric("ì¤‘ì•™ê°’ í•´ìƒë„", resolution_info["ì¤‘ì•™ê°’"])
                        st.info(f"í‰ê·  í”½ì…€ ìˆ˜: {resolution_info['í‰ê·  í”½ì…€ ìˆ˜']} í”½ì…€")
                        # í•´ìƒë„ ëª©ë¡ í‘œì‹œ (í™•ì¥ ê°€ëŠ¥) - ìš”ì•½ ì •ë³´ë§Œ
                        with st.expander("ì„ íƒëœ ì´ë¯¸ì§€ë“¤ì˜ ì‹¤ì œ í•´ìƒë„ ëª©ë¡ (ì „ì²´ ë³´ê¸°)", expanded=False):
                            if "í•´ìƒë„ ëª©ë¡" in results:
                                # í•´ìƒë„ë³„ ê·¸ë£¹í™”í•˜ì—¬ í‘œì‹œ
                                from collections import Counter
                                resolution_counts = Counter(results["í•´ìƒë„ ëª©ë¡"])
                                st.write("**í•´ìƒë„ë³„ ê°œìˆ˜:**")
                                for res, count in sorted(resolution_counts.items(), key=lambda x: x[1], reverse=True):
                                    st.write(f"- {res}: {count}ê°œ")
                            # ì „ì²´ ëª©ë¡ì€ ë„ˆë¬´ ê¸¸ì–´ì„œ ì œì™¸ (ìš”ì•½ ì •ë³´ë§Œ í‘œì‹œ)
                            total_count = len(results.get('í•´ìƒë„ ëª©ë¡', []))
                            if total_count > 0:
                                st.info(f"ì´ {total_count}ê°œ ì´ë¯¸ì§€ì˜ í•´ìƒë„ ì •ë³´ (ìƒì„¸ ëª©ë¡ì€ ìƒëµ)")
                    # ìƒ˜í”Œ ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸° (ì „ì²´ í‘œì‹œ)
                    if len(images) > 0:
                        st.subheader(f"ì„ íƒëœ ì´ë¯¸ì§€ ì „ì²´ ({len(images)}ê°œ)")
                        # 5ì—´ ê·¸ë¦¬ë“œë¡œ í‘œì‹œ
                        num_cols = 5
                        num_rows = (len(images) + num_cols - 1) // num_cols  # ì˜¬ë¦¼ ê³„ì‚°
                        for row in range(num_rows):
                            cols = st.columns(num_cols)
                            for col_idx in range(num_cols):
                                img_idx = row * num_cols + col_idx
                                if img_idx < len(images):
                                    with cols[col_idx]:
                                        st.image(images[img_idx], use_container_width=True)
                                        if "í•´ìƒë„ ëª©ë¡" in results and img_idx < len(results["í•´ìƒë„ ëª©ë¡"]):
                                            st.caption(f"#{img_idx+1} ({results['í•´ìƒë„ ëª©ë¡'][img_idx]})")
                                        else:
                                            st.caption(f"#{img_idx+1}")
                    # ê°œë³„ ì ìˆ˜ í‘œì‹œ (í† ê¸€)
                    if "ê°œë³„ ì ìˆ˜" in results and len(results["ê°œë³„ ì ìˆ˜"]) > 0:
                        with st.expander("ê°œë³„ ì´ë¯¸ì§€ ì ìˆ˜ ìƒì„¸ ë³´ê¸°", expanded=False):
                            import pandas as pd
                            # ê°œë³„ ì ìˆ˜ ë°ì´í„°í”„ë ˆì„ ìƒì„±
                            df_scores = pd.DataFrame(results["ê°œë³„ ì ìˆ˜"])
                            df_scores.index = df_scores.index + 1  # ì¸ë±ìŠ¤ë¥¼ 1ë¶€í„° ì‹œì‘
                            df_scores.index.name = "ì´ë¯¸ì§€ ë²ˆí˜¸"
                            # ì •ë ¬ ì˜µì…˜ ì¶”ê°€
                            col_sort1, col_sort2 = st.columns(2)
                            with col_sort1:
                                sort_by = st.selectbox("ì •ë ¬ ê¸°ì¤€", ["ì¢…í•©ì ìˆ˜", "í•´ìƒë„", "ìœ íš¨ì„±"], key="img_sort")
                            with col_sort2:
                                sort_order = st.selectbox("ì •ë ¬ ë°©í–¥", ["ë‚´ë¦¼ì°¨ìˆœ", "ì˜¤ë¦„ì°¨ìˆœ"], key="img_order")
                            ascending = (sort_order == "ì˜¤ë¦„ì°¨ìˆœ")
                            # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ ì •ë ¬
                            if sort_by in df_scores.columns:
                                df_scores_sorted = df_scores.sort_values(by=sort_by, ascending=ascending)
                            else:
                                st.warning(f"âš ï¸ ì •ë ¬ ê¸°ì¤€ '{sort_by}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì •ë ¬ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                                df_scores_sorted = df_scores.sort_values(by="ì¢…í•©ì ìˆ˜", ascending=False)
                            # ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
                            st.dataframe(df_scores_sorted, use_container_width=True)
                            # í†µê³„ ìš”ì•½
                            st.caption(f"ì´ {len(df_scores)}ê°œ ì´ë¯¸ì§€ | í‰ê·  ì¢…í•©ì ìˆ˜: {results['í‰ê·  ì¢…í•© ì ìˆ˜']:.3f} | ìµœì†Œ: {results['ìµœì†Œ ì¢…í•© ì ìˆ˜']:.3f} | ìµœëŒ€: {results['ìµœëŒ€ ì¢…í•© ì ìˆ˜']:.3f}")
                    # PDF ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                    st.divider()
                    pdf_buffer = generate_dataset_report_pdf(results, "ì´ë¯¸ì§€", dataset_option)
                    filename = f"image_dataset_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
                    st.download_button(
                        label="ğŸ“„ PDF ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ",
                        data=pdf_buffer.getvalue(),
                        file_name=filename,
                        mime="application/pdf",
                        type="primary",
                        use_container_width=True
                    )
                # í…ìŠ¤íŠ¸ ë¶„ì„
                elif texts:
                    # ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    results = analyze_dataset_texts(texts, max_samples=len(texts))
                    progress_bar.progress(100)
                    status_text.text("ë¶„ì„ ì™„ë£Œ!")
                    # ê²°ê³¼ í‘œì‹œ
                    col1, col2 = st.columns(2)
                    with col1:
                        st.subheader("ì „ì²´ í†µê³„")
                        # ê¸´ ë¦¬ìŠ¤íŠ¸ëŠ” ì œì™¸í•˜ê³  ìš”ì•½ ì •ë³´ë§Œ í‘œì‹œ
                        filtered_results = {}
                        exclude_keys = ["ê°œë³„ ì ìˆ˜"]  # ë„ˆë¬´ ê¸´ ë¦¬ìŠ¤íŠ¸ ì œì™¸
                        for key, value in results.items():
                            if key not in exclude_keys:
                                # ê¸´ ë¦¬ìŠ¤íŠ¸ëŠ” ê°œìˆ˜ë§Œ í‘œì‹œ
                                if isinstance(value, list) and len(value) > 20:
                                    filtered_results[key] = f"{len(value)}ê°œ í•­ëª©"
                                else:
                                    filtered_results[key] = value
                        st.dataframe({
                            "ì§€í‘œ": list(filtered_results.keys()),
                            "ê°’": [str(v) for v in filtered_results.values()]
                        },
                        use_container_width=True
                        )
                    with col2:
                        avg_score = results.get("í‰ê·  ì¢…í•© ì ìˆ˜", 0.0)
                        grade = get_grade(avg_score)
                        st.metric("í‰ê·  í’ˆì§ˆ ì ìˆ˜", f"{avg_score:.3f}")
                        st.metric("í’ˆì§ˆ ë“±ê¸‰", grade)
                        if grade == "A":
                            st.success("ìš°ìˆ˜í•œ í’ˆì§ˆì˜ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤!")
                        elif grade == "B":
                            st.info("ì–‘í˜¸í•œ í’ˆì§ˆì˜ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.")
                        elif grade == "C":
                            st.warning("í’ˆì§ˆ ê°œì„ ì´ ê¶Œì¥ë©ë‹ˆë‹¤.")
                        else:
                            st.error("í’ˆì§ˆ ê°œì„ ì´ ì‹œê¸‰í•©ë‹ˆë‹¤.")
                    # ìƒì„¸ ì§€í‘œ ì‹œê°í™” (ë©”ì¸)
                    st.subheader("í’ˆì§ˆ ì§€í‘œ ìƒì„¸")
                    metrics_data = {
                        "í‰ê·  í˜•ì‹ ì •í™•ì„±": results["í‰ê·  í˜•ì‹ ì •í™•ì„±"],
                        "í‰ê·  ë‹¤ì–‘ì„±": results["í‰ê·  ë‹¤ì–‘ì„±"],
                        "í‰ê·  ì™„ì „ì„±": results["í‰ê·  ì™„ì „ì„±"],
                    }
                    st.bar_chart(metrics_data)
                    # ê°œë³„ ì ìˆ˜ í‘œì‹œ (í† ê¸€)
                    if "ê°œë³„ ì ìˆ˜" in results and len(results["ê°œë³„ ì ìˆ˜"]) > 0:
                        with st.expander("ê°œë³„ í…ìŠ¤íŠ¸ ì ìˆ˜ ìƒì„¸ ë³´ê¸°", expanded=False):
                            import pandas as pd
                            # ê°œë³„ ì ìˆ˜ ë°ì´í„°í”„ë ˆì„ ìƒì„±
                            df_scores = pd.DataFrame(results["ê°œë³„ ì ìˆ˜"])
                            df_scores.index = df_scores.index + 1  # ì¸ë±ìŠ¤ë¥¼ 1ë¶€í„° ì‹œì‘
                            df_scores.index.name = "í…ìŠ¤íŠ¸ ë²ˆí˜¸"
                            # ì •ë ¬ ì˜µì…˜ ì¶”ê°€
                            col_sort1, col_sort2 = st.columns(2)
                            with col_sort1:
                                sort_by = st.selectbox("ì •ë ¬ ê¸°ì¤€", ["ì¢…í•©ì ìˆ˜", "í˜•ì‹ ì •í™•ì„±", "ë‹¤ì–‘ì„±", "ì™„ì „ì„±"], key="text_sort")
                            with col_sort2:
                                sort_order = st.selectbox("ì •ë ¬ ë°©í–¥", ["ë‚´ë¦¼ì°¨ìˆœ", "ì˜¤ë¦„ì°¨ìˆœ"], key="text_order")
                            ascending = (sort_order == "ì˜¤ë¦„ì°¨ìˆœ")
                            # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ ì •ë ¬
                            if sort_by in df_scores.columns:
                                df_scores_sorted = df_scores.sort_values(by=sort_by, ascending=ascending)
                            else:
                                st.warning(f"âš ï¸ ì •ë ¬ ê¸°ì¤€ '{sort_by}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì •ë ¬ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                                df_scores_sorted = df_scores.sort_values(by="ì¢…í•©ì ìˆ˜", ascending=False)
                            # ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
                            st.dataframe(df_scores_sorted, use_container_width=True)
                            # í†µê³„ ìš”ì•½
                            st.caption(f"ì´ {len(df_scores)}ê°œ í…ìŠ¤íŠ¸ | í‰ê·  ì¢…í•©ì ìˆ˜: {results['í‰ê·  ì¢…í•© ì ìˆ˜']:.3f} | ìµœì†Œ: {results['ìµœì†Œ ì¢…í•© ì ìˆ˜']:.3f} | ìµœëŒ€: {results['ìµœëŒ€ ì¢…í•© ì ìˆ˜']:.3f}")
                    # ì„ íƒëœ í…ìŠ¤íŠ¸ ì „ì²´ í‘œì‹œ (í† ê¸€)
                    if len(texts) > 0:
                        with st.expander(f"ì„ íƒëœ í…ìŠ¤íŠ¸ ì „ì²´ ë³´ê¸° ({len(texts)}ê°œ)", expanded=False):
                            for i, text in enumerate(texts):
                                # ê°œë³„ ì ìˆ˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                                score_info = ""
                                if "ê°œë³„ ì ìˆ˜" in results and i < len(results["ê°œë³„ ì ìˆ˜"]):
                                    score = results["ê°œë³„ ì ìˆ˜"][i]
                                    score_info = f" | í˜•ì‹ ì •í™•ì„±: {score.get('í˜•ì‹ ì •í™•ì„±', 0):.3f}, ë‹¤ì–‘ì„±: {score.get('ë‹¤ì–‘ì„±', 0):.3f}, ì™„ì „ì„±: {score.get('ì™„ì „ì„±', 0):.3f}, ì¢…í•©: {score.get('ì¢…í•©ì ìˆ˜', 0):.3f}"
                                with st.expander(f"í…ìŠ¤íŠ¸ #{i+1} (ê¸¸ì´: {len(text)}ì{score_info})", expanded=False):
                                    st.text(text[:1000] + "..." if len(text) > 1000 else text)
                    # PDF ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                    st.divider()
                    pdf_buffer = generate_dataset_report_pdf(results, "í…ìŠ¤íŠ¸", dataset_option)
                    filename = f"text_dataset_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
                    st.download_button(
                        label="ğŸ“„ PDF ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ",
                        data=pdf_buffer.getvalue(),
                        file_name=filename,
                        mime="application/pdf",
                        type="primary",
                        use_container_width=True
                    )
        except ImportError as e:
            st.error(f"í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n`pip install torchvision`ì„ ì‹¤í–‰í•˜ì„¸ìš”.\n\nì—ëŸ¬: {e}")
        except FileNotFoundError as e:
            st.error(f"ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nì—ëŸ¬: {e}")
        except Exception as e:
            st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\nì—ëŸ¬: {e}")
            st.exception(e)
    # ì‚¬ìš© ê°€ì´ë“œ
    with st.expander("ë°ì´í„°ì…‹ ë¶„ì„ ê°€ì´ë“œ"):
        st.markdown("""
        ### ì§€ì› íŒŒì¼ í˜•ì‹
        - **í…ìŠ¤íŠ¸**: `.txt` íŒŒì¼
        - **ì´ë¯¸ì§€**: `.jpg`, `.jpeg`, `.png`, `.gif`, `.bmp`
        ### í’ˆì§ˆ ì§€í‘œ ì„¤ëª…
        #### í…ìŠ¤íŠ¸ ë°ì´í„°
        - **í˜•ì‹ ì •í™•ì„±**: ì˜¤íƒˆì ë° ë§ì¶¤ë²• ì˜¤ë¥˜ ë¹„ìœ¨
        - **ë‹¤ì–‘ì„±**: ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ë¶„ì„ (ì¤‘ë³µì´ ì ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ë†’ìŒ)
        - **ì™„ì „ì„±**: ì˜ë¯¸ ìˆëŠ” ë¬¸ì¥ì˜ ë¹„ìœ¨
        #### ì´ë¯¸ì§€ ë°ì´í„°
        - **í•´ìƒë„**: ì´ë¯¸ì§€ í¬ê¸° ê¸°ì¤€ ì¶©ì¡± ì—¬ë¶€
        - **ìœ íš¨ì„±**: ì´ë¯¸ì§€ í’ˆì§ˆ (ì„ ëª…ë„ ë° ë…¸ì´ì¦ˆ í†µí•© ì§€í‘œ)
        - **ë‹¤ì–‘ì„±**: ì¤‘ë³µ ì´ë¯¸ì§€ ë¹„ìœ¨ (ì¤‘ë³µì´ ì ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ë†’ìŒ)
        ### í’ˆì§ˆ ë“±ê¸‰
        - **A**: 0.8 ì´ìƒ (ìš°ìˆ˜)
        - **B**: 0.6 ì´ìƒ (ì–‘í˜¸)
        - **C**: 0.4 ì´ìƒ (ë³´í†µ)
        - **D**: 0.4 ë¯¸ë§Œ (ê°œì„  í•„ìš”)
        """)
